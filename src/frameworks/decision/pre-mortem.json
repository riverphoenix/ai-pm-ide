{
  "id": "pre-mortem",
  "name": "Pre-Mortem Analysis",
  "category": "decision",
  "description": "Identify what could go wrong before launch by imagining failure and working backward",
  "icon": "⚠️",
  "example_output": "# Pre-Mortem Analysis: AI PM Assistant Tool - V1 Launch\n\n## Setup\n\n**Project**: Public launch of AI PM Assistant Tool V1\n**Imagined Failure Date**: 3 months post-launch (May 2026)\n**Failure Statement**: \"It is May 2026. Our V1 launch has failed. We have 200 paying users instead of our target of 5,000. Churn is 40% monthly. We are burning $80K/month with no clear path to sustainability. The team is demoralized and debating whether to pivot or shut down.\"\n\n**Participants**: Product Lead, Engineering Manager, Tech Lead, UX Lead, Marketing Lead, Data Lead\n**Session Date**: 2026-02-15\n\n---\n\n## Failure Modes Identified\n\n### Category 1: Product & Quality\n\n#### FM-1: AI Output Quality Is Inconsistent\n- **What happened**: Users found that the AI-generated frameworks were generic, repetitive, and often missed the nuance of their specific context. Power users compared outputs to ChatGPT with a good prompt template and saw no meaningful difference.\n- **Likelihood**: High (4/5)\n- **Impact**: Critical (5/5)\n- **Risk Score**: 20\n- **Early Warning Signs**: Low repeat usage in Week 1; users copy-pasting outputs into ChatGPT to \"fix\" them; NPS below 20\n- **Mitigation**:\n  - Invest 2 sprints in prompt engineering with real PM feedback loops before launch\n  - Build a quality evaluation pipeline: 100 test prompts scored by 5 PMs weekly\n  - Implement user feedback thumbs up/down on every output with optional comments\n  - Set launch gate: 80% of test outputs rated \"good\" or \"excellent\" by PM evaluators\n\n#### FM-2: Framework Coverage Is Too Narrow\n- **What happened**: Users arrived expecting to use frameworks we do not support. The top 3 missing frameworks (Kano Model, Wardley Mapping, Design Sprint Planning) were requested by 30% of trial users. They left disappointed.\n- **Likelihood**: Medium (3/5)\n- **Impact**: High (4/5)\n- **Risk Score**: 12\n- **Early Warning Signs**: Support tickets asking for specific frameworks; \"missing feature\" as top churn reason in exit surveys\n- **Mitigation**:\n  - Survey 200 PMs pre-launch to identify top 20 most-wanted frameworks\n  - Ensure coverage of at least 15 of the top 20 before launch\n  - Build a public framework request board with voting to prioritize post-launch additions\n\n### Category 2: Technical & Infrastructure\n\n#### FM-3: API Costs Spiral Beyond Projections\n- **What happened**: Average user consumed 3x more tokens than modeled. At 2,000 users, API costs hit $45K/month against $15K projected. The $25/month price point became unsustainable.\n- **Likelihood**: Medium (3/5)\n- **Impact**: Critical (5/5)\n- **Risk Score**: 15\n- **Early Warning Signs**: Token consumption per session trending above model; cost per user exceeding $5/month in beta\n- **Mitigation**:\n  - Implement token budgets per user tier from day one\n  - Build aggressive caching: identical framework requests serve cached results\n  - Use smaller, cheaper models for simple tasks (formatting, suggestions) and reserve large models for generation\n  - Set automated alerts when cost-per-user exceeds $4/month\n\n#### FM-4: Reliability Issues During Peak Usage\n- **What happened**: Product Hunt launch drove 10x normal traffic. The AI provider rate-limited us. 60% of users on launch day saw timeout errors. First impressions were ruined and negative reviews spread on Twitter.\n- **Likelihood**: Medium (3/5)\n- **Impact**: Critical (5/5)\n- **Risk Score**: 15\n- **Early Warning Signs**: Load test failures at 5x capacity; no fallback provider configured; rate limit headroom below 2x\n- **Mitigation**:\n  - Load test at 20x expected launch day traffic\n  - Implement multi-provider fallback (primary: Claude, fallback: GPT-5)\n  - Add request queuing with user-facing progress indicator instead of timeout errors\n  - Negotiate rate limit increases with AI providers 4 weeks before launch\n\n### Category 3: Go-to-Market & Growth\n\n#### FM-5: Positioning Fails to Differentiate\n- **What happened**: Users perceived us as \"just another ChatGPT wrapper.\" The value of structured PM frameworks was not communicated effectively. Marketing focused on AI features rather than PM workflow outcomes.\n- **Likelihood**: High (4/5)\n- **Impact**: High (4/5)\n- **Risk Score**: 16\n- **Early Warning Signs**: Landing page conversion below 3%; \"how is this different from ChatGPT?\" in user interviews; low engagement with framework-specific features\n- **Mitigation**:\n  - Lead marketing with workflow outcomes, not AI technology (\"Ship better PRDs in 10 minutes\" not \"AI-powered document generation\")\n  - Create 5 comparison pages showing side-by-side output quality vs. raw ChatGPT\n  - Partner with 3 PM influencers for authentic testimonials before launch\n\n#### FM-6: Free Tier Is Too Generous\n- **What happened**: 95% of users stayed on the free tier. The 3 free frameworks per month was enough for casual users, and power users found workarounds (multiple accounts). Conversion to paid was 1.5% vs. 8% target.\n- **Likelihood**: Medium (3/5)\n- **Impact**: High (4/5)\n- **Risk Score**: 12\n- **Early Warning Signs**: Free-to-paid conversion below 4% in first month; power users averaging exactly 3 uses per month (hitting limit then stopping)\n- **Mitigation**:\n  - Limit free tier to 2 frameworks per month (not 3) with watermarked exports\n  - Gate advanced features (export to Notion, version history, team sharing) behind paid tier\n  - Implement annual pricing with 40% discount to increase commitment\n\n### Category 4: Team & Execution\n\n#### FM-7: Team Burns Out Before Launch\n- **What happened**: The 6-week launch cycle had no slack. Two engineers went on unplanned leave. Scope was not cut, so remaining team worked 60+ hour weeks. Quality suffered, bugs shipped, and morale cratered post-launch.\n- **Likelihood**: Medium (3/5)\n- **Impact**: High (4/5)\n- **Risk Score**: 12\n- **Early Warning Signs**: Sprint velocity declining over 3 consecutive sprints; team members skipping standups or 1:1s; increasing bug count in code reviews\n- **Mitigation**:\n  - Define a \"must-have\" vs. \"nice-to-have\" scope list now, with pre-agreed cut line\n  - Build 1 week of buffer into the 6-week cycle (5 weeks work + 1 week contingency)\n  - Establish a \"circuit breaker\" rule: if any team member works >50 hours for 2 consecutive weeks, auto-trigger a scope cut discussion\n\n---\n\n## Risk Ranking Summary\n\n| Rank | Risk | Score (L x I) | Category |\n|------|------|---------------|----------|\n| 1 | FM-1: AI Output Quality | 20 | Product |\n| 2 | FM-5: Positioning Failure | 16 | Go-to-Market |\n| 3 | FM-3: API Cost Spiral | 15 | Technical |\n| 4 | FM-4: Reliability at Launch | 15 | Technical |\n| 5 | FM-2: Framework Coverage Gaps | 12 | Product |\n| 6 | FM-6: Free Tier Too Generous | 12 | Go-to-Market |\n| 7 | FM-7: Team Burnout | 12 | Team |\n\n---\n\n## Top 3 Action Items\n\n1. **Invest in AI output quality evaluation pipeline** (Owner: Tech Lead, Due: Week 2) - This is the single biggest risk to the entire product. Ship quality gates before shipping features.\n2. **Rewrite launch positioning around outcomes, not technology** (Owner: Marketing Lead, Due: Week 3) - Every asset must answer \"why not just use ChatGPT?\" convincingly.\n3. **Load test and implement multi-provider fallback** (Owner: Tech Lead, Due: Week 4) - A failed launch day cannot be recovered. Invest in resilience now.",
  "system_prompt": "You are a risk analysis expert specializing in pre-mortem exercises. The pre-mortem technique, developed by psychologist Gary Klein, asks teams to imagine that a project has already failed and then work backward to identify what went wrong. This overcomes optimism bias and surfaces risks that traditional planning misses.\n\nFollow this methodology:\n\n## 1. Set the Scene\nFrame the exercise by stating the project, the imagined failure date (typically 3-6 months post-launch), and a vivid 2-3 sentence failure statement. The failure statement should be specific and visceral enough to overcome the team's natural optimism. Include concrete numbers where possible (e.g., user targets missed, revenue shortfall, team attrition).\n\n## 2. Identify Failure Modes\nGenerate 5-8 distinct failure modes organized into categories:\n- **Product & Quality**: Features that missed the mark, poor UX, quality issues\n- **Technical & Infrastructure**: Scalability failures, outages, security breaches, cost overruns\n- **Go-to-Market & Growth**: Positioning failures, channel underperformance, pricing mistakes\n- **Team & Execution**: Burnout, turnover, communication breakdowns, scope creep\n- **External & Market**: Competitor moves, regulatory changes, market shifts\n\nFor each failure mode, write a narrative describing what happened as if looking back from the failure date. Make it concrete and specific, not abstract.\n\n## 3. Score Each Risk\nFor each failure mode, assess:\n- **Likelihood**: 1-5 scale (1=Very Unlikely, 5=Almost Certain)\n- **Impact**: 1-5 scale (1=Minor, 5=Critical/Existential)\n- **Risk Score**: Likelihood x Impact\n- **Early Warning Signs**: What leading indicators would signal this risk is materializing?\n\n## 4. Define Mitigations\nFor each failure mode, provide 2-4 specific, actionable mitigation steps. Each mitigation should be concrete enough to become a task or user story. Avoid generic advice like \"communicate better\" - instead specify what, how, when, and who.\n\n## 5. Prioritize\nRank all failure modes by Risk Score (highest first). Identify the top 3 that demand immediate action. For each top risk, specify an owner and a deadline for the first mitigation step.\n\nFormat the output with clear markdown headers, tables for scoring summaries, and bold for emphasis. Use narrative storytelling for failure descriptions to make risks feel real and urgent.",
  "guiding_questions": [
    "What project or launch are you running the pre-mortem for?",
    "What does success look like? What are the specific targets (users, revenue, timeline)?",
    "What is the timeline for this project?",
    "What are you most worried about already?",
    "Who are the key people involved and what are the team dynamics?",
    "Have there been any similar past projects that struggled or failed?",
    "What external factors (market, competitors, regulations) could affect this?"
  ],
  "supports_visuals": false
}
