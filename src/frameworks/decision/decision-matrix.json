{
  "id": "decision-matrix",
  "name": "Multi-Criteria Decision Matrix",
  "category": "decision",
  "description": "Evaluate options against weighted criteria to make objective, data-driven decisions",
  "icon": "ðŸŽ²",
  "example_output": "# Multi-Criteria Decision Matrix: Selecting an AI Provider for PM Assistant Tool\n\n## Decision Context\n\n**Decision**: Which AI provider should power the core intelligence layer of our PM assistant tool?\n\n**Decision Owner**: VP of Engineering\n**Stakeholders**: Product Lead, CTO, Finance Director\n**Timeline**: Decision needed by end of sprint (2 weeks)\n**Constraints**: Budget ceiling of $50K/month in API costs at scale; must support <2s response latency; must comply with SOC 2 requirements.\n\n---\n\n## Options Under Evaluation\n\n| # | Option | Description |\n|---|--------|-------------|\n| A | **OpenAI GPT-5** | Latest flagship model with strong general reasoning and broad ecosystem |\n| B | **Anthropic Claude Opus** | Advanced model with strong instruction-following and safety features |\n| C | **Google Gemini Ultra** | Multimodal model with deep Google Cloud integration |\n| D | **Self-hosted Llama 4** | Open-source model deployed on own infrastructure |\n\n---\n\n## Criteria Definition & Weighting\n\n| # | Criterion | Weight | Rationale |\n|---|-----------|--------|-----------|\n| 1 | Output Quality for PM Tasks | 30% | Core value proposition depends on high-quality framework generation |\n| 2 | API Reliability & Latency | 20% | Users expect real-time responses; downtime erodes trust |\n| 3 | Cost at Scale (100K users) | 20% | Unit economics must support $25/mo price point |\n| 4 | Data Privacy & Compliance | 15% | Enterprise customers require SOC 2, GDPR compliance |\n| 5 | Integration Flexibility | 10% | Must support streaming, function calling, fine-tuning |\n| 6 | Vendor Lock-in Risk | 5% | Ability to switch providers without major refactoring |\n\n**Total Weight**: 100%\n\n---\n\n## Scoring Matrix\n\nScale: 1 (Poor) to 5 (Excellent)\n\n| Criterion (Weight) | GPT-5 (A) | Claude Opus (B) | Gemini Ultra (C) | Llama 4 Self-hosted (D) |\n|---------------------|-----------|-----------------|-------------------|------------------------|\n| Output Quality (30%) | 5 | 5 | 4 | 3 |\n| Reliability & Latency (20%) | 4 | 4 | 4 | 3 |\n| Cost at Scale (20%) | 3 | 3 | 3 | 5 |\n| Privacy & Compliance (15%) | 3 | 4 | 3 | 5 |\n| Integration Flexibility (10%) | 5 | 4 | 4 | 3 |\n| Vendor Lock-in Risk (5%) | 2 | 2 | 2 | 5 |\n\n---\n\n## Weighted Scores\n\n| Criterion (Weight) | GPT-5 (A) | Claude Opus (B) | Gemini Ultra (C) | Llama 4 (D) |\n|---------------------|-----------|-----------------|-------------------|-----------|\n| Output Quality (30%) | 1.50 | 1.50 | 1.20 | 0.90 |\n| Reliability & Latency (20%) | 0.80 | 0.80 | 0.80 | 0.60 |\n| Cost at Scale (20%) | 0.60 | 0.60 | 0.60 | 1.00 |\n| Privacy & Compliance (15%) | 0.45 | 0.60 | 0.45 | 0.75 |\n| Integration Flexibility (10%) | 0.50 | 0.40 | 0.40 | 0.30 |\n| Vendor Lock-in Risk (5%) | 0.10 | 0.10 | 0.10 | 0.25 |\n| **TOTAL** | **3.95** | **4.00** | **3.55** | **3.80** |\n\n---\n\n## Ranking\n\n| Rank | Option | Weighted Score |\n|------|--------|----------------|\n| 1 | **Claude Opus (B)** | 4.00 |\n| 2 | GPT-5 (A) | 3.95 |\n| 3 | Llama 4 Self-hosted (D) | 3.80 |\n| 4 | Gemini Ultra (C) | 3.55 |\n\n---\n\n## Sensitivity Analysis\n\n- If **Cost** weight increases from 20% to 35%: Llama 4 moves to #1 (4.05) due to zero API costs\n- If **Output Quality** weight increases from 30% to 45%: GPT-5 and Claude tie at first, Llama 4 drops significantly\n- If **Privacy** weight increases from 15% to 25%: Claude Opus remains #1, Llama 4 moves to #2\n\nThe top two options (Claude Opus and GPT-5) are within 0.05 points, indicating a near-tie. The decision is robust unless cost becomes the dominant concern.\n\n---\n\n## Recommendation\n\n**Primary choice: Anthropic Claude Opus (B)** with a score of 4.00\n\n**Rationale**: Claude Opus edges ahead due to its stronger privacy and compliance posture, which will be critical for enterprise sales. Output quality is on par with GPT-5 for structured PM tasks. The 0.05 point difference over GPT-5 is small, but the compliance advantage provides meaningful differentiation for our target market.\n\n**Mitigation**: Implement a multi-provider abstraction layer from day one. This addresses vendor lock-in risk and allows fallback to GPT-5 or Llama 4 if pricing, availability, or quality shifts.\n\n**Next Steps**:\n1. Negotiate enterprise agreement with Anthropic for volume pricing\n2. Build provider abstraction layer (2 sprints)\n3. Run parallel quality evaluation with 500 real PM prompts across both top providers\n4. Revisit decision in 6 months based on actual usage data",
  "system_prompt": "You are a decision analysis expert specializing in multi-criteria decision matrices. Your role is to help teams make objective, structured decisions by evaluating options against clearly defined and weighted criteria.\n\nFollow this methodology to produce the decision matrix:\n\n## 1. Decision Context\nStart by clearly framing the decision. State the specific question being answered, the decision owner, key stakeholders, timeline, and any hard constraints or non-negotiables that apply. This ensures alignment before analysis begins.\n\n## 2. Options Inventory\nList all viable options under consideration. For each option, provide a brief one-line description. Aim for 3-6 options. If more than 6 exist, suggest pre-screening to narrow down before the full matrix analysis.\n\n## 3. Criteria Definition & Weighting\nIdentify 4-8 evaluation criteria that matter for this decision. For each criterion, assign a percentage weight reflecting its relative importance. Weights must sum to 100%. Provide a brief rationale for each weight. Criteria should be specific, measurable where possible, and collectively cover the key dimensions of the decision (e.g., cost, quality, risk, speed, strategic fit).\n\n## 4. Scoring\nScore each option against each criterion on a consistent 1-5 scale (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent). Be explicit about what each score means in context. Avoid defaulting everything to 3-4; use the full range where warranted.\n\n## 5. Weighted Score Calculation\nMultiply each raw score by the criterion weight to get weighted scores. Sum weighted scores per option to get the total. Present results in a clear table.\n\n## 6. Sensitivity Analysis\nTest the robustness of the result by asking: What happens if key weights change by 10-15 percentage points? Identify which weight changes would flip the ranking. This reveals whether the decision is clear-cut or fragile.\n\n## 7. Recommendation\nState the recommended option with its total score. Explain why it wins, noting key differentiators. Acknowledge close alternatives and suggest mitigations for any weaknesses of the chosen option. Provide concrete next steps.\n\nFormat the entire output in clean markdown with tables, headers, bold for emphasis, and a logical flow from context through analysis to recommendation.",
  "guiding_questions": [
    "What specific decision are you trying to make?",
    "What are the options you are evaluating?",
    "What criteria matter most for this decision (e.g., cost, quality, speed, risk)?",
    "Are there any non-negotiable requirements or hard constraints?",
    "Who are the key stakeholders and what are their priorities?",
    "What is the timeline for making this decision?",
    "Do you have any existing data or scores for the options?"
  ],
  "supports_visuals": false
}
