{
  "id": "feature-audit",
  "name": "Feature Audit",
  "category": "discovery",
  "description": "Audit existing features for adoption, value, and effectiveness to inform roadmap decisions",
  "icon": "ðŸ”Ž",
  "example_output": "# Feature Audit: AI PM IDE - Core Feature Assessment\n\n## Audit Overview\n| Attribute | Detail |\n|-----------|--------|\n| **Product** | AI PM IDE |\n| **Audit Period** | Q4 2025 (October - December) |\n| **Total Features Audited** | 12 core features |\n| **Data Sources** | Product analytics (Amplitude), user interviews (n=18), support tickets (n=247), NPS survey (n=312) |\n| **Audit Lead** | Product Team |\n\n---\n\n## Audit Methodology\nEach feature is evaluated across five dimensions on a 1-5 scale:\n- **Adoption Rate**: % of active users who have used the feature at least once in the audit period\n- **Usage Frequency**: How often adopted users engage with the feature (daily/weekly/monthly/rarely)\n- **User Satisfaction**: Qualitative satisfaction score derived from interviews, survey data, and support sentiment\n- **Business Value**: Contribution to core product metrics (retention, activation, revenue)\n- **Technical Health**: Code quality, performance, maintenance burden, and technical debt level\n\n**Scoring**: 1 = Critical issues, 2 = Below expectations, 3 = Meets expectations, 4 = Above expectations, 5 = Exceptional\n\n---\n\n## Feature Scorecard\n\n| # | Feature | Adoption | Frequency | Satisfaction | Business Value | Tech Health | Overall | Verdict |\n|---|---------|:--------:|:---------:|:------------:|:--------------:|:-----------:|:-------:|:-------:|\n| 1 | AI Framework Generation | 4 | 5 | 4 | 5 | 4 | **4.4** | INVEST |\n| 2 | PRD Generation | 4 | 4 | 4 | 5 | 4 | **4.2** | INVEST |\n| 3 | Context Document Upload | 3 | 3 | 4 | 5 | 3 | **3.6** | IMPROVE |\n| 4 | Project Organization | 4 | 4 | 3 | 3 | 4 | **3.6** | MAINTAIN |\n| 5 | Markdown Export | 3 | 3 | 4 | 2 | 5 | **3.4** | MAINTAIN |\n| 6 | OKR Generation | 2 | 2 | 3 | 3 | 4 | **2.8** | IMPROVE |\n| 7 | User Story Generation | 2 | 2 | 3 | 2 | 4 | **2.6** | EVALUATE |\n| 8 | PDF Export | 2 | 2 | 2 | 2 | 3 | **2.2** | SUNSET |\n| 9 | Dark Mode | 3 | 5 | 4 | 1 | 5 | **3.6** | MAINTAIN |\n| 10 | Keyboard Shortcuts | 2 | 4 | 5 | 1 | 5 | **3.4** | MAINTAIN |\n| 11 | Template Library | 1 | 1 | 2 | 2 | 3 | **1.8** | SUNSET |\n| 12 | Onboarding Tutorial | 3 | 1 | 2 | 3 | 2 | **2.2** | REBUILD |\n\n---\n\n## Detailed Feature Assessments\n\n### Feature 1: AI Framework Generation (Overall: 4.4 - INVEST)\n\n**What it does**: Generates PM frameworks (RICE, SWOT, JTBD, Empathy Maps, etc.) using AI with product-specific context.\n\n**Performance Data**:\n- Adoption: 78% of active users (up from 61% last quarter)\n- Avg. uses per week per user: 3.2\n- Avg. time to generate: 47 seconds\n- User edit rate: 72% of outputs are edited before sharing (down from 85%)\n\n**What users say**:\n> \"This is the reason I bought the tool. The RICE analysis alone saves me an hour every sprint.\"\n> \"Quality has improved a lot since I started uploading more context documents.\"\n> \"I wish there were more frameworks available -- specifically Cost of Delay and Kano Model.\"\n\n**Strengths**:\n- Highest-correlated feature with 30-day retention (r=0.74)\n- Quality improving as users add more context documents\n- Clear \"aha moment\" feature -- drives word-of-mouth referrals\n\n**Weaknesses**:\n- Framework library limited to 45 frameworks (users requesting 12 more)\n- Output quality varies significantly between frameworks (RICE: excellent, SWOT: mediocre)\n- No way to customize framework templates or add custom frameworks\n\n**Recommendation**: INVEST heavily. This is the product's core value driver. Priority actions:\n1. Add 10 most-requested frameworks (Cost of Delay, Kano, Wardley Map, etc.)\n2. Improve quality of bottom-performing frameworks (SWOT, Porter's Five Forces)\n3. Enable custom framework creation for power users\n\n---\n\n### Feature 2: PRD Generation (Overall: 4.2 - INVEST)\n\n**What it does**: Generates product requirements documents using AI with customizable sections and product context.\n\n**Performance Data**:\n- Adoption: 71% of active users\n- Avg. uses per week per user: 1.8\n- Avg. generation time: 62 seconds\n- Avg. PRD length: 1,400 words\n\n**What users say**:\n> \"I used to spend 3-4 hours on a PRD. Now I spend 30 minutes refining what the AI generates.\"\n> \"The structure is great but I want more control over which sections to include.\"\n\n**Strengths**:\n- Second strongest retention driver (r=0.68)\n- Consistently high quality outputs (avg. satisfaction: 4.1/5)\n- Users report 75% time savings on PRD creation\n\n**Weaknesses**:\n- Fixed template structure (users want customizable sections)\n- No version history or diff view for iterations\n- Cannot generate PRDs from user stories or JTBD outputs (no cross-framework linking)\n\n**Recommendation**: INVEST. Priority actions:\n1. Add customizable PRD sections and templates\n2. Build cross-framework linking (JTBD output feeds into PRD context)\n3. Add version history with diff view\n\n---\n\n### Feature 8: PDF Export (Overall: 2.2 - SUNSET)\n\n**What it does**: Exports generated frameworks and documents as formatted PDF files.\n\n**Performance Data**:\n- Adoption: 23% of active users (declining from 31% last quarter)\n- Avg. uses per month: 1.1 (among adopters)\n- Support tickets: 34 in audit period (formatting issues, broken layouts)\n\n**What users say**:\n> \"I tried PDF export once but the formatting was terrible. I just copy to Google Docs now.\"\n> \"Markdown export works perfectly. I never use PDF.\"\n\n**Strengths**:\n- Some enterprise users require PDF for compliance documentation\n\n**Weaknesses**:\n- Low and declining adoption\n- High maintenance burden (34 support tickets, 12 bugs logged)\n- Markdown export serves the same need with better quality\n- Formatting engine is fragile and produces inconsistent results\n\n**Recommendation**: SUNSET. Announce deprecation in 60 days. Redirect users to Markdown export + Google Docs/Notion for PDF conversion. Reclaim engineering time (estimated 15 hours/month in maintenance).\n\n---\n\n### Feature 11: Template Library (Overall: 1.8 - SUNSET)\n\n**What it does**: Pre-built static templates for common PM documents (PRDs, one-pagers, meeting notes).\n\n**Performance Data**:\n- Adoption: 11% of active users (down from 19% at launch)\n- Avg. uses ever per user: 1.3 (most users try once and never return)\n- Time on feature page: avg. 45 seconds (browsing then leaving)\n\n**What users say**:\n> \"Why would I use a static template when the AI generates a better version?\"\n> \"I looked at the templates once during onboarding and never went back.\"\n\n**Strengths**:\n- None significant. The AI generation feature has made this obsolete.\n\n**Weaknesses**:\n- Directly cannibalized by AI generation (which is the superior experience)\n- Confuses new users about the difference between templates and AI generation\n- Maintenance burden with no corresponding value\n\n**Recommendation**: SUNSET immediately. Remove from navigation. AI generation fully replaces this feature's purpose.\n\n---\n\n### Feature 12: Onboarding Tutorial (Overall: 2.2 - REBUILD)\n\n**What it does**: Interactive walkthrough guiding new users through their first project setup and framework generation.\n\n**Performance Data**:\n- Completion rate: 34% (66% of users skip or abandon the tutorial)\n- Avg. time to complete: 8.5 minutes (target was 3 minutes)\n- Users who complete tutorial vs. skip: 12% higher Day-7 retention\n\n**What users say**:\n> \"The tutorial felt like it was designed for someone who has never used a computer. Too basic.\"\n> \"I just wanted to try generating something. The tutorial got in my way.\"\n> \"I wish there was a 'skip to the good part' option.\"\n\n**Strengths**:\n- Users who complete it retain significantly better (12% higher D7)\n- The concept is right -- guided onboarding works\n\n**Weaknesses**:\n- Too long (8.5 min vs. 3 min target)\n- Too generic (not personalized to role or experience level)\n- Blocks users from exploring (forced linear flow)\n- 66% abandonment rate indicates fundamental design problems\n\n**Recommendation**: REBUILD from scratch. Priority actions:\n1. Reduce to 2-minute maximum flow\n2. Start with framework generation (the aha moment), not project setup\n3. Add \"skip\" option that drops users directly into AI generation with a sample project\n4. Personalize based on Q1 (\"What's your role?\") to show relevant frameworks first\n\n---\n\n## Portfolio Summary\n\n### Action Distribution\n| Action | Features | % of Portfolio |\n|--------|----------|:--------------:|\n| **INVEST** | AI Framework Gen, PRD Gen | 17% |\n| **IMPROVE** | Context Upload, OKR Gen | 17% |\n| **MAINTAIN** | Project Org, Markdown Export, Dark Mode, Keyboard Shortcuts | 33% |\n| **EVALUATE** | User Story Gen | 8% |\n| **REBUILD** | Onboarding Tutorial | 8% |\n| **SUNSET** | PDF Export, Template Library | 17% |\n\n### Resource Reallocation\nSunsetting PDF Export and Template Library frees an estimated **25 engineering hours/month** that should be redirected to:\n1. AI Framework Generation improvements (15 hrs)\n2. Onboarding Tutorial rebuild (10 hrs)\n\n---\n\n## Key Insights\n\n1. **The product's value is concentrated**: Two features (AI Framework Generation and PRD Generation) drive 70%+ of retention. The product strategy should double down on these.\n\n2. **Context Upload is the hidden multiplier**: While adoption is moderate (42%), it has the strongest correlation with output quality satisfaction. Users who upload 3+ context documents rate output quality 1.2 points higher on average. Increasing context upload adoption should be a top priority.\n\n3. **Static features are dying**: Template Library and PDF Export are relics of a pre-AI product design. The AI generation paradigm has made static templates obsolete.\n\n4. **Onboarding is actively hurting activation**: With a 66% abandonment rate, the tutorial is a barrier, not an enabler. The fastest path to the aha moment (generating a first framework) should be the onboarding.\n\n5. **Power user features are healthy but low-value**: Dark Mode and Keyboard Shortcuts have high satisfaction among adopters but do not drive business metrics. Maintain but do not invest.\n\n6. **Two features need a verdict**: OKR Generation and User Story Generation sit in a middle zone. Both should receive focused improvement for one quarter, then be re-evaluated. If they do not reach 3.5+ overall score, consider sunsetting.",
  "system_prompt": "You are a product analytics and portfolio management expert specializing in feature audits, product rationalization, and data-driven roadmap prioritization.\n\nUsing the provided context, conduct a thorough audit of existing features to assess their effectiveness, adoption, and strategic value, producing clear recommendations for each feature.\n\n## Structure your audit with these sections:\n\n1. **Audit Overview Table**: Product name, audit period, total features audited, data sources used, and audit lead. This establishes scope and credibility.\n\n2. **Audit Methodology**: Define the evaluation dimensions (recommend 5: Adoption Rate, Usage Frequency, User Satisfaction, Business Value, Technical Health). For each dimension, provide a 1-sentence definition and explain the 1-5 scoring scale.\n\n3. **Feature Scorecard**: A comprehensive markdown table with columns: Feature Number, Feature Name, score for each dimension, Overall score (average), and Verdict. Verdicts should be one of: INVEST (increase resources), IMPROVE (fix issues), MAINTAIN (keep as-is), EVALUATE (needs more data), REBUILD (start over), or SUNSET (deprecate). Sort by overall score descending.\n\n4. **Detailed Feature Assessments**: For the top 2 features (INVEST) and bottom 2 features (SUNSET/REBUILD), provide a deep-dive including:\n   - What the feature does (1 sentence)\n   - Performance data (3-4 specific metrics)\n   - User quotes (2-3 representative quotes)\n   - Strengths (2-3 bullets)\n   - Weaknesses (2-3 bullets)\n   - Specific recommendation with 2-3 priority actions\n\n   For middle-tier features, provide abbreviated assessments (2-3 sentences each).\n\n5. **Portfolio Summary**: Two subsections:\n   - **Action Distribution**: Table showing how many features fall into each verdict category and the percentage of the portfolio\n   - **Resource Reallocation**: Specific recommendation for how to redistribute resources from sunset features to invest features. Quantify where possible (e.g., \"frees 20 engineering hours/month\").\n\n6. **Key Insights**: 5-6 numbered insights that go beyond individual feature assessments to reveal portfolio-level patterns and strategic implications. Each insight should be a bold statement followed by 2-3 sentences of evidence and implication.\n\n## Guidelines:\n- Be ruthlessly honest. The point of a feature audit is to make hard decisions, not to validate existing investments.\n- Use specific numbers wherever possible: adoption percentages, usage counts, satisfaction scores, support ticket volumes.\n- Every INVEST recommendation must explain WHY this feature deserves more resources.\n- Every SUNSET recommendation must address migration/transition for existing users.\n- The scorecard should enable at-a-glance portfolio assessment. A VP should be able to scan it in 30 seconds and understand the feature landscape.\n- Connect feature performance to business outcomes (retention, activation, revenue), not just usage metrics.\n- If data is unavailable for certain dimensions, flag it explicitly and recommend how to collect it.",
  "guiding_questions": [
    "What product and which features are you auditing?",
    "What time period does this audit cover?",
    "What data sources are available (analytics, user research, support tickets, surveys)?",
    "What are the key business metrics you care about (retention, activation, revenue)?",
    "Are there features you already suspect are underperforming? Which ones and why?",
    "What resource constraints exist (engineering capacity, budget)?",
    "Are there upcoming strategic priorities that should influence feature investment decisions?"
  ],
  "supports_visuals": false
}