{
  "id": "retrospective",
  "name": "Sprint/Project Retrospective",
  "category": "execution",
  "description": "Structured retrospective to reflect on what worked, what didn't, and actions to improve",
  "icon": "ðŸ”„",
  "example_output": "# Sprint Retrospective: AI PM Workspace - Sprint 7\n\n## Sprint Overview\n- **Sprint**: Sprint 7 (Jan 20 - Feb 2, 2026)\n- **Team**: Core Product (6 engineers, 1 PM, 1 designer)\n- **Sprint Goal**: Launch context-aware framework generation with document upload\n- **Sprint Goal Met**: Partially (MVP launched, but quality below target)\n- **Velocity**: 34 story points completed / 42 planned (81%)\n- **Facilitator**: Sarah Chen (PM)\n- **Date of Retro**: Feb 3, 2026\n\n---\n\n## What Went Well\n\nThese are the practices, decisions, and outcomes we want to celebrate and continue.\n\n### 1. Cross-functional collaboration on prompt engineering\nThe AI/ML engineer paired directly with the designer to iterate on prompt templates. This produced significantly better framework outputs than either could have achieved alone. The designer brought structure and formatting expertise while the engineer tuned the model parameters.\n- **Impact**: Output quality scores improved from 3.2 to 3.8 during the sprint.\n- **Action**: Continue pairing on all prompt engineering work.\n\n### 2. Early user testing with beta cohort\nWe tested document upload with 8 beta users in Week 1 instead of waiting until the end. Three critical UX issues were caught and fixed before wider release.\n- **Impact**: Avoided a poor launch experience; saved an estimated 1-week rework cycle.\n- **Action**: Maintain weekly beta testing sessions.\n\n### 3. Technical spike on PDF parsing saved time\nThe two-day spike on PDF parsing libraries in Sprint 6 paid off. The team implemented document ingestion in 3 days instead of the estimated 5 because the approach was already validated.\n- **Impact**: 2 days saved on implementation.\n- **Action**: Continue investing in spikes for technically uncertain work.\n\n### 4. Standup format change improved focus\nSwitching from round-robin status updates to \"blockers first\" standup format cut meeting time from 20 minutes to 10 minutes and surfaced blockers faster.\n- **Impact**: Team saved 50 minutes/week collectively; blockers resolved same-day.\n- **Action**: Keep the new standup format permanently.\n\n---\n\n## What Didn't Go Well\n\nThese are the areas where we struggled, made mistakes, or fell short of expectations.\n\n### 1. Framework output quality did not meet the 4.0/5.0 target\nDespite improvements, we landed at 3.8/5.0 against a 4.0 target. The primary issue was that generated frameworks lacked specificity when users provided minimal context. The system defaulted to generic outputs.\n- **Root Cause**: Insufficient fallback logic when context documents are sparse or low-quality.\n- **Impact**: 22% of beta users rated outputs as \"too generic\" in the feedback survey.\n\n### 2. Scope creep on the settings page\nWhat started as a simple \"upload preferences\" page expanded to include notification settings, API key management, and theme configuration. None of these were in the original sprint plan.\n- **Root Cause**: No clear scope boundary was set; the engineer saw adjacent work and took initiative without checking priority.\n- **Impact**: 8 story points of unplanned work; contributed to missing the sprint goal.\n\n### 3. Late discovery of API rate limits in production\nThe document parsing API had a 100 requests/minute rate limit that was not discovered until the staging deployment. This required an emergency caching layer.\n- **Root Cause**: No load testing was done during the spike; API docs were not reviewed for rate limits.\n- **Impact**: 1.5 days of unplanned emergency work; delayed QA by one day.\n\n### 4. Unclear acceptance criteria on two stories\nTwo user stories (\"As a PM, I want to upload documents\" and \"As a PM, I want better output quality\") had vague acceptance criteria. This led to different interpretations between the engineer and QA.\n- **Root Cause**: Stories were written quickly during sprint planning without the usual refinement session.\n- **Impact**: 2 bugs escaped to staging; 4 hours of rework.\n\n---\n\n## Key Metrics Comparison\n\n| Metric | Sprint 6 | Sprint 7 | Delta | Assessment |\n|---|---|---|---|---|\n| Velocity (points completed) | 38 | 34 | -10.5% | Declined due to scope creep |\n| Sprint goal completion | 100% | 81% | -19% | Partially met |\n| Bug escape rate | 1 | 3 | +200% | Needs attention |\n| Cycle time (avg days/story) | 2.8 | 3.4 | +21% | Slower due to unclear AC |\n| Team satisfaction (1-5) | 4.1 | 3.7 | -0.4 | Lower due to unplanned work |\n| Beta user satisfaction | 3.4/5 | 3.8/5 | +0.4 | Improving steadily |\n\n---\n\n## Action Items\n\nConcreted, assigned actions with deadlines. Each action addresses a specific issue identified above.\n\n| # | Action | Owner | Deadline | Addresses |\n|---|---|---|---|---|\n| 1 | Implement fallback prompt logic for low-context scenarios | Alex (AI/ML) | Sprint 8, Week 1 | Output quality gap |\n| 2 | Add \"scope boundary\" section to every sprint plan document | Sarah (PM) | Next sprint planning | Scope creep |\n| 3 | Add rate limit check to the technical spike template | James (Tech Lead) | Sprint 8, Day 1 | API rate limit surprise |\n| 4 | Reinstate mandatory story refinement session (Thursday before sprint) | Sarah (PM) | Ongoing | Unclear acceptance criteria |\n| 5 | Create a load testing checklist for all third-party API integrations | James (Tech Lead) | Sprint 8, Week 1 | API rate limit surprise |\n\n---\n\n## Team Mood Check\n\nAnonymous pulse survey results (1 = Very Unhappy, 5 = Very Happy):\n\n| Dimension | Score | Trend |\n|---|---|---|\n| Overall happiness | 3.7/5 | Slightly down |\n| Workload sustainability | 3.2/5 | Down (unplanned work impact) |\n| Psychological safety | 4.5/5 | Stable (team shares openly) |\n| Clarity of direction | 3.8/5 | Stable |\n| Confidence in next sprint | 4.0/5 | Up (clear actions defined) |\n\n---\n\n## Follow-Up from Previous Retrospective\n\n| Previous Action | Status | Notes |\n|---|---|---|\n| Implement standup format change | Done | Working well, keeping it |\n| Add monitoring dashboard for AI generation | Done | Dashboard live, used daily |\n| Schedule weekly beta user sessions | Done | 8 users tested this sprint |",
  "system_prompt": "You are an Agile coaching expert specializing in effective sprint and project retrospectives that drive continuous improvement.\n\nYour task is to facilitate a structured retrospective based on the user's input about their recent sprint, project, or cycle.\n\n## Methodology:\n\nUse a structured retrospective format that goes beyond simple \"what went well / what didn't\" by adding root cause analysis, metrics comparison, and concrete action items.\n\n1. **Sprint Overview**: Summarize the sprint or project context including goal, team composition, duration, velocity, and whether the goal was met.\n\n2. **What Went Well (4-6 items)**: Identify specific practices, decisions, and outcomes worth celebrating. For each item:\n   - Describe what happened concretely (not vaguely)\n   - Explain the impact it had\n   - Recommend whether to continue, amplify, or formalize this practice\n\n3. **What Didn't Go Well (4-6 items)**: Identify specific problems, failures, and friction points. For each item:\n   - Describe what happened with specifics\n   - Identify the root cause (use 5 Whys thinking)\n   - Quantify the impact where possible (time lost, bugs introduced, morale hit)\n\n4. **Key Metrics Comparison**: Create a table comparing this sprint/cycle's metrics against the previous one. Include velocity, goal completion, bug escape rate, cycle time, and team satisfaction.\n\n5. **Action Items**: Convert insights into 4-6 concrete, assigned actions. Each action must have:\n   - A clear owner (specific person, not \"the team\")\n   - A deadline (specific date or sprint milestone)\n   - A link back to which issue it addresses\n   Actions must be specific and achievable within one sprint.\n\n6. **Team Mood Check**: Include a pulse survey section covering happiness, workload sustainability, psychological safety, clarity, and confidence.\n\n7. **Follow-Up from Previous Retro**: Track the status of action items from the last retrospective to ensure accountability.\n\n## Best Practices:\n- Be honest but constructive; the goal is improvement, not blame\n- Focus on systemic issues, not individual mistakes\n- Every problem identified must result in at least one action item\n- Action items without owners and deadlines are wishes, not commitments\n- Include both quantitative metrics and qualitative team sentiment\n- Celebrate wins genuinely; positive reinforcement drives engagement\n- Keep the retro forward-looking: what will we do differently next time?\n- Use markdown tables for structured data and clear formatting",
  "guiding_questions": [
    "What was the sprint or project goal, and was it achieved?",
    "What is your team composition and sprint duration?",
    "What were 3-4 things that went particularly well this cycle?",
    "What were 3-4 things that caused frustration, delays, or quality issues?",
    "What were your key metrics this sprint (velocity, bugs, cycle time)?",
    "Were there any action items from the previous retrospective, and what is their status?",
    "How would you describe the overall team morale and energy level?"
  ],
  "supports_visuals": false
}
