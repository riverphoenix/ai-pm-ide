{
  "id": "success-metrics",
  "name": "Success Metrics Definition",
  "category": "execution",
  "description": "Define measurable success criteria with leading/lagging indicators and measurement plans",
  "icon": "ðŸ“ˆ",
  "example_output": "# Success Metrics Definition: AI PM Workspace\n\n## Initiative Overview\n- **Initiative**: Launch context-aware AI framework generation\n- **Product**: AI-powered Product Management Workspace\n- **Owner**: Sarah Chen, Product Manager\n- **Timeframe**: Q1 2026 (January - March)\n- **Objective**: Validate that AI-generated PM frameworks, enhanced with user-uploaded product context, deliver meaningfully better outputs that drive habitual usage.\n\n---\n\n## Success Statement\n\nThis initiative is successful if users regularly generate high-quality PM frameworks using their own product context, resulting in measurably faster artifact creation, strong retention, and organic word-of-mouth growth. We will know we have succeeded when context-enhanced generation becomes the default behavior for active users.\n\n---\n\n## Primary Success Metrics\n\nThese are the 3 metrics that directly measure whether this initiative achieved its goal.\n\n### 1. Framework Output Quality Score\n- **Definition**: Average user rating (1-5 scale) on AI-generated framework outputs when product context documents are uploaded\n- **Measurement Method**: In-app rating prompt after each generation (\"How useful was this output?\")\n- **Baseline**: 3.2/5.0 (without context documents)\n- **Target**: 4.2/5.0 (with context documents)\n- **Minimum Viable Threshold**: 3.8/5.0 (below this, the feature does not add sufficient value)\n- **Data Source**: Mixpanel event tracking + PostgreSQL ratings table\n- **Measurement Frequency**: Daily aggregation, weekly review\n\n### 2. Context Upload Adoption Rate\n- **Definition**: Percentage of active users who upload at least one context document to a project within their first 14 days\n- **Measurement Method**: Funnel analysis: account creation > project creation > document upload\n- **Baseline**: 0% (feature not yet launched)\n- **Target**: 45% of active users\n- **Minimum Viable Threshold**: 25% (below this, discoverability or value proposition is unclear)\n- **Data Source**: Mixpanel funnel + S3 upload logs\n- **Measurement Frequency**: Weekly cohort analysis\n\n### 3. Weekly Frameworks Generated per Active User\n- **Definition**: Average number of frameworks generated per user who had at least one session that week\n- **Measurement Method**: Count of generation events / weekly active users\n- **Baseline**: 1.2 frameworks/user/week (current, without context feature)\n- **Target**: 2.8 frameworks/user/week\n- **Minimum Viable Threshold**: 2.0 frameworks/user/week\n- **Data Source**: Mixpanel product analytics\n- **Measurement Frequency**: Weekly\n\n---\n\n## Leading Indicators\n\nThese early signals will tell us within the first 2-4 weeks whether we are on track toward our primary metrics.\n\n| Indicator | What It Signals | Target | Check-In |\n|---|---|---|---|\n| Day 1 context upload rate | Is the upload flow discoverable and intuitive? | >15% of new users | Week 1 |\n| Avg. documents uploaded per project | Are users investing in context quality? | >2.0 docs/project | Week 2 |\n| Generation completion rate (with context) | Does context-enhanced generation work reliably? | >95% | Week 1 |\n| Time to first context-enhanced generation | How fast do users experience the core value? | <8 minutes from signup | Week 2 |\n| Qualitative feedback sentiment | What are users saying in feedback forms? | >60% positive | Week 3 |\n| Support ticket volume related to uploads | Is the feature causing confusion? | <5 tickets/week | Week 2 |\n\n---\n\n## Lagging Indicators\n\nThese metrics confirm long-term success and will become meaningful 4-8 weeks after launch.\n\n| Indicator | What It Confirms | Target | Check-In |\n|---|---|---|---|\n| Day 30 retention (context users vs. non-context) | Does context usage drive stickiness? | Context users: 35% vs. Non-context: 18% | Week 8 |\n| NPS score (context users) | Are context users more satisfied? | NPS >= 50 for context users | Week 8 |\n| Organic referral rate | Are users recommending the feature? | 8% of new signups from referral links | Week 10 |\n| Free-to-paid conversion (context users) | Does context usage drive monetization? | Context users convert at 2x rate of non-context | Week 12 |\n\n---\n\n## Guardrail Metrics\n\nMetrics that must NOT degrade as we pursue success. If any guardrail is breached, we pause and investigate before continuing.\n\n| Guardrail | Threshold | Why It Matters | Escalation |\n|---|---|---|---|\n| Overall generation success rate | Must not drop below 92% | Adding context parsing must not break the core flow | Immediate engineering fix; rollback if unresolved in 24h |\n| P95 generation latency | Must stay under 8 seconds | Context processing must not make the product feel slow | Performance optimization sprint |\n| LLM cost per generation | Must stay under $0.10 | Context-enhanced prompts are larger; costs must remain viable | Implement caching or prompt compression |\n| Non-context generation quality | Must not decrease from 3.2 baseline | Improving context mode must not degrade the default experience | Investigate prompt regression |\n\n---\n\n## Measurement Plan\n\n### Data Collection\n\n| Data Point | Collection Method | Instrumentation Needed | Owner |\n|---|---|---|---|\n| Framework generation events | Mixpanel track event | Add context_documents_count property | Engineering |\n| Output quality ratings | In-app survey component | Build rating modal post-generation | Product + Engineering |\n| Document upload events | S3 upload webhook + Mixpanel | Track upload_success, upload_failure, doc_type | Engineering |\n| Support tickets | Intercom tagging | Add \"context-upload\" tag category | Support |\n| User interviews | Calendly scheduling | Recruit 10 beta users for Week 3 interviews | Product |\n\n### Analysis Schedule\n\n| Timeframe | Analysis | Owner | Output |\n|---|---|---|---|\n| Daily | Monitor guardrail metrics dashboard | Engineering on-call | Slack alert if breached |\n| Weekly | Review leading indicators, update scorecard | PM (Sarah) | Weekly metrics email to team |\n| Bi-weekly | Cohort analysis: context users vs. non-context | Data Analyst | Retention and conversion comparison |\n| Monthly | Full success metrics review with leadership | PM + Head of Product | Go/no-go decision on doubling down or pivoting |\n| End of Quarter | Comprehensive initiative retrospective | PM + full team | Success/failure determination; next steps |\n\n---\n\n## Decision Framework\n\nHow we will use these metrics to make decisions:\n\n| Scenario | Criteria | Decision |\n|---|---|---|\n| Strong success | All 3 primary metrics hit target; guardrails intact | Double down: increase investment, expand to more frameworks |\n| Partial success | 2 of 3 primary metrics hit minimum threshold; guardrails intact | Iterate: investigate the underperforming metric; run experiments |\n| Underperformance | Fewer than 2 primary metrics hit minimum threshold | Pivot: conduct user research to understand why; redefine approach |\n| Guardrail breach | Any guardrail metric crosses threshold | Pause: stop new development; fix the issue before proceeding |\n\n---\n\n## Stakeholder Reporting\n\n| Audience | What They See | Cadence | Format |\n|---|---|---|---|\n| Engineering team | Guardrails + leading indicators | Daily (Slack) | Automated dashboard alerts |\n| Product team | All metrics + qualitative feedback | Weekly | Metrics scorecard email |\n| Leadership | Primary metrics + decision recommendation | Bi-weekly | Slide deck (3 slides) |\n| Board / Investors | Primary metrics + strategic narrative | Quarterly | Investor update section |",
  "system_prompt": "You are a product analytics and measurement expert specializing in defining clear, actionable success metrics for product initiatives and features.\n\nYour task is to help the user define a complete success metrics framework for their initiative, ensuring they can objectively determine whether the work achieved its goals.\n\n## Methodology:\n\nFollow a structured approach to success metrics definition:\n\n1. **Initiative Overview**: Clearly state what is being measured, who owns it, the timeframe, and the core objective in one sentence.\n\n2. **Success Statement**: Write a 2-3 sentence narrative description of what success looks like. This should be understandable by anyone in the company.\n\n3. **Primary Success Metrics (3 metrics)**: Define exactly 3 metrics that directly measure whether the initiative achieved its goal. For each metric, specify:\n   - Precise definition (no ambiguity in how it is calculated)\n   - Measurement method (how data is collected)\n   - Current baseline\n   - Target value\n   - Minimum viable threshold (the floor below which the initiative is considered unsuccessful)\n   - Data source\n   - Measurement frequency\n\n4. **Leading Indicators (5-7 metrics)**: Early signals visible within 2-4 weeks that predict whether primary metrics will be hit. Include target values and the week by which each should be checked.\n\n5. **Lagging Indicators (3-5 metrics)**: Long-term confirmation metrics that validate sustained success after 4-12 weeks. These confirm the initiative created lasting value.\n\n6. **Guardrail Metrics (3-5 metrics)**: Metrics that must NOT degrade while pursuing success. Define hard thresholds and escalation actions if breached.\n\n7. **Measurement Plan**: Detail the data collection infrastructure needed, instrumentation requirements, ownership, and analysis schedule from daily monitoring to quarterly review.\n\n8. **Decision Framework**: Define explicit criteria for four scenarios: strong success, partial success, underperformance, and guardrail breach. Each scenario should map to a clear decision (double down, iterate, pivot, or pause).\n\n9. **Stakeholder Reporting**: Define what each audience sees, how often, and in what format.\n\n## Best Practices:\n- Every metric must have a precise, unambiguous definition. If two people could calculate it differently, it is not well-defined.\n- Always include a minimum viable threshold, not just a target. This separates \"stretch goal\" from \"failure.\"\n- Leading indicators are essential for early course correction. Do not skip them.\n- Guardrail metrics prevent unintended consequences and gaming. Always include them.\n- The measurement plan must be realistic. If you cannot instrument a metric, you cannot use it.\n- Include both quantitative metrics and qualitative signals (user interviews, feedback sentiment).\n- The decision framework removes ambiguity: before you launch, agree on what each outcome means.\n- Use markdown tables extensively for structured, scannable output.",
  "guiding_questions": [
    "What initiative or feature are you defining success metrics for?",
    "What is the core objective â€” what does success look like in one sentence?",
    "What metrics do you currently have baselines for, and what is new?",
    "What is your timeframe for evaluating success (weeks, months, quarters)?",
    "What could go wrong â€” what negative side effects should we watch for?",
    "What tools and data infrastructure do you have for measurement (analytics, surveys, etc.)?",
    "Who are the stakeholders who need to see these metrics, and what decisions will they make?"
  ],
  "supports_visuals": false
}
