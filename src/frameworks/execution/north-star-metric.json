{
  "id": "north-star-metric",
  "name": "North Star Metric",
  "category": "execution",
  "description": "Identify and align your organization around a single metric that drives sustainable growth",
  "icon": "⭐",
  "example_output": "# North Star Metric Definition: AI PM Workspace\n\n## Executive Summary\nAfter analyzing our product's value delivery, growth loops, and business model, we have identified **\"Weekly Frameworks Generated per Active User\"** as our North Star Metric. This metric captures the core moment users extract value from our platform and directly correlates with retention, expansion revenue, and long-term sustainable growth.\n\n---\n\n## North Star Metric\n\n### Metric: Weekly Frameworks Generated per Active User\n- **Current Value**: 1.2 frameworks/user/week\n- **6-Month Target**: 3.5 frameworks/user/week\n- **12-Month Target**: 5.0 frameworks/user/week\n\n### Why This Metric?\nThis metric was chosen because it sits at the intersection of three critical dimensions:\n1. **Value Delivery**: Each framework generated represents a tangible decision or artifact the PM created faster with our tool. More frameworks = more value extracted.\n2. **Revenue Correlation**: Users generating 3+ frameworks/week have a 92% monthly retention rate vs. 34% for users generating <1. Power users convert to paid plans at 4.7x the rate.\n3. **Product Vision Alignment**: Our mission is to make every PM 10x more effective. Framework generation frequency is the purest signal of whether we are achieving that mission.\n\n---\n\n## Input Metrics (Leading Indicators)\n\nThese are the levers we can pull to move the North Star Metric:\n\n### 1. Context Documents Uploaded per Project\n- **Current**: 1.8 docs/project\n- **Target**: 4.0 docs/project\n- **Owner**: Product Team\n- **Rationale**: More context leads to higher-quality outputs, which drives repeat usage. Users who upload 3+ documents generate 2.4x more frameworks.\n\n### 2. Framework Output Quality Score\n- **Current**: 3.6/5.0 (user rating)\n- **Target**: 4.3/5.0\n- **Owner**: AI/ML Team\n- **Rationale**: Higher quality outputs reduce the need for manual editing and increase trust, leading to more frequent generation.\n\n### 3. Time to First Framework (Onboarding)\n- **Current**: 12 minutes\n- **Target**: 4 minutes\n- **Owner**: Growth Team\n- **Rationale**: Faster time-to-value in onboarding increases activation rate. Users who generate their first framework within 5 minutes have 68% higher Week 4 retention.\n\n### 4. Framework Template Coverage\n- **Current**: 28 templates across 5 categories\n- **Target**: 60 templates across 8 categories\n- **Owner**: Content Team\n- **Rationale**: Broader template coverage means the tool is useful in more PM scenarios, driving habitual usage.\n\n### 5. Share & Collaboration Rate\n- **Current**: 18% of frameworks shared\n- **Target**: 45% of frameworks shared\n- **Owner**: Product Team\n- **Rationale**: Shared frameworks create viral loops and team adoption, increasing per-user generation across the organization.\n\n---\n\n## Anti-Metrics (Guardrails)\n\nMetrics we must monitor to ensure we are not gaming the North Star:\n\n| Anti-Metric | Threshold | Why It Matters |\n|---|---|---|\n| Framework abandonment rate | Must stay below 20% | If users start but don't finish, we are pushing quantity over quality |\n| Average edit rate post-generation | Must stay below 40% | Excessive editing means AI output quality is declining |\n| Support ticket volume | Must not increase >10% MoM | Growth should not come at the cost of confusion |\n| LLM cost per framework | Must stay below $0.08 | Unit economics must remain viable as usage scales |\n\n---\n\n## Alignment Across Teams\n\n| Team | How They Contribute | Their Input Metric |\n|---|---|---|\n| Product | Improve generation UX, reduce friction | Time to generate, completion rate |\n| AI/ML | Improve output quality and relevance | Quality score, edit rate |\n| Growth | Drive activation and onboarding | Time to first framework, Day 7 retention |\n| Content | Expand template library | Template count, category coverage |\n| Sales | Drive team adoption for enterprise | Seats per account, team usage rate |\n\n---\n\n## Validation Plan\n\n### How We Know This Is the Right Metric\n1. **Correlation Analysis**: Run correlation between framework generation frequency and 90-day retention, NRR, and NPS scores.\n2. **A/B Testing**: For the next quarter, run experiments that target moving input metrics and measure impact on the North Star.\n3. **Quarterly Review**: Re-evaluate the North Star Metric every quarter. If correlation to revenue weakens, reassess.\n\n### Review Cadence\n- **Weekly**: Dashboard review in Monday standup (input metrics + North Star)\n- **Monthly**: Deep dive with leadership on trends, experiments, and learnings\n- **Quarterly**: Full North Star Metric validation and potential recalibration",
  "system_prompt": "You are a growth strategy and product analytics expert specializing in North Star Metric identification and alignment.\n\nYour task is to help the user identify their product's North Star Metric (NSM) and build a complete alignment framework around it.\n\n## Methodology:\n\nFollow the North Star Framework popularized by Amplitude and Sean Ellis:\n\n1. **Identify the Core Value Moment**: What is the single action that represents a user getting real value from the product? This is not a vanity metric (pageviews, signups) but a deep engagement signal.\n\n2. **Validate Against Three Dimensions**:\n   - **Value Delivery**: Does this metric reflect real value to the user?\n   - **Revenue Correlation**: Does increasing this metric correlate with revenue growth?\n   - **Product Vision Alignment**: Does this metric point toward the long-term product vision?\n\n3. **Define Input Metrics**: Identify 4-6 leading indicators (input metrics) that are actionable levers teams can directly influence to move the North Star. Each input metric should have a clear owner, current baseline, and target.\n\n4. **Set Anti-Metrics (Guardrails)**: Define 3-5 metrics that must NOT degrade as the North Star improves. These prevent gaming and ensure sustainable, healthy growth.\n\n5. **Map Team Alignment**: Show how every team in the organization contributes to the North Star through their specific input metrics.\n\n6. **Create a Validation Plan**: Define how the team will confirm the NSM is the right metric, including correlation analysis, experimentation, and review cadence.\n\n## Structure the Output As:\n- Executive Summary (2-3 sentences)\n- North Star Metric definition with current value and targets\n- Rationale (why this metric, validated against three dimensions)\n- Input Metrics table (4-6 metrics with owners and targets)\n- Anti-Metrics / Guardrails table\n- Team Alignment map\n- Validation Plan with review cadence\n\n## Best Practices:\n- A good NSM is measurable, actionable, and reflects customer value\n- Avoid revenue as the NSM (it is a lagging indicator); instead find what drives revenue\n- The NSM should be understandable by every person in the company\n- Input metrics should be within a team's direct control\n- Include specific numbers, baselines, and targets wherever possible\n- Use markdown tables for clarity and structure",
  "guiding_questions": [
    "What is the core action a user takes when they get real value from your product?",
    "What is your current business model and primary revenue driver?",
    "What does your retention curve look like — where do users drop off?",
    "What metrics do you currently track, and which ones correlate most with retention or revenue?",
    "How many distinct teams or functions need to align around growth?",
    "What is your current growth stage (pre-launch, early traction, scaling, mature)?",
    "What does your ideal engaged user look like — what do they do weekly?"
  ],
  "supports_visuals": false
}
