{
  "id": "technical-spec",
  "name": "Technical Specification",
  "category": "development",
  "description": "Detailed technical design document covering architecture, APIs, data models, and implementation",
  "icon": "\ud83d\udd27",
  "example_output": "# Technical Specification: AI Framework Recommendation Engine\n\n## 1. Summary\n\n**Author**: Sarah Lopez, Backend Engineer\n**Reviewers**: David Park (Tech Lead), James Kim (Frontend), Maria Chen (Full-Stack)\n**Status**: Draft\n**Last Updated**: February 17, 2026\n\nThis document describes the technical design for the AI-powered framework recommendation engine in the PM Workspace. The engine analyzes a user's project context (description, uploaded documents, selected category) and returns ranked framework suggestions with confidence scores.\n\n---\n\n## 2. Background & Motivation\n\nThe PM Workspace currently offers 45+ framework templates across 7 categories. User research shows that 68% of users browse for more than 3 minutes before selecting a framework, and 23% abandon the flow entirely. An intelligent recommendation system will reduce selection time to under 30 seconds and increase framework adoption by an estimated 40%.\n\n**Related Documents**:\n- PRD: Framework Recommendation Engine (PM-201)\n- User Research: Framework Discovery Pain Points (Feb 2026)\n\n---\n\n## 3. Goals & Non-Goals\n\n**Goals**:\n- Return top 3 framework recommendations within 2 seconds of request\n- Achieve recommendation relevance score above 0.7 (measured by user acceptance rate)\n- Support both text input and uploaded document context\n- Graceful degradation when LLM API is unavailable\n\n**Non-Goals**:\n- Real-time collaborative recommendation (single-user only for V1)\n- Custom user-trained models (using pre-built LLM with prompt engineering)\n- Recommendation history or analytics dashboard\n\n---\n\n## 4. Architecture Overview\n\n### System Context\n\nThe recommendation engine operates as a backend service that sits between the frontend UI and the LLM provider APIs. It receives project context from the client, constructs an optimized prompt, calls the LLM API, parses the response, and returns structured recommendations.\n\n### Component Diagram\n\n```\n[Frontend UI] --> [API Gateway] --> [Recommendation Service]\n                                         |\n                                    [Prompt Builder]\n                                         |\n                                    [LLM Provider]\n                                    (Claude / GPT-4)\n                                         |\n                                    [Response Parser]\n                                         |\n                                    [Framework Matcher]\n                                         |\n                                    [Cache Layer (Redis)]\n```\n\n### Key Components\n\n1. **Recommendation Service**: Orchestrates the recommendation flow. Receives project context, delegates to Prompt Builder, calls LLM, and returns structured results.\n\n2. **Prompt Builder**: Constructs the LLM prompt from project context. Includes framework metadata (names, categories, descriptions) and user input. Optimizes token usage by truncating context to 2,000 tokens max.\n\n3. **Response Parser**: Extracts structured data (framework IDs, confidence scores, reasoning) from LLM free-text response. Uses JSON-mode where supported, falls back to regex extraction.\n\n4. **Framework Matcher**: Validates that returned framework IDs exist in the library. Handles edge cases where the LLM hallucinates framework names.\n\n5. **Cache Layer**: Redis-based cache keyed on hashed project context. TTL of 1 hour. Reduces API costs by ~30% based on projected usage patterns.\n\n---\n\n## 5. API Design\n\n### POST /api/v1/recommendations\n\n**Request**:\n```json\n{\n  \"project_id\": \"uuid\",\n  \"context\": {\n    \"description\": \"string (max 5000 chars)\",\n    \"category_hint\": \"string | null\",\n    \"documents\": [\n      {\n        \"name\": \"string\",\n        \"content_summary\": \"string (max 1000 chars)\"\n      }\n    ]\n  },\n  \"max_results\": 3\n}\n```\n\n**Response (200)**:\n```json\n{\n  \"recommendations\": [\n    {\n      \"framework_id\": \"sprint-planning\",\n      \"framework_name\": \"Sprint Planning\",\n      \"confidence\": 0.92,\n      \"reasoning\": \"Project involves iterative development with a cross-functional team and 2-week cycles.\"\n    }\n  ],\n  \"model_used\": \"claude-3-opus\",\n  \"latency_ms\": 1240,\n  \"cached\": false\n}\n```\n\n**Error Responses**:\n- `400`: Invalid request (missing context, exceeds character limits)\n- `429`: Rate limit exceeded (max 10 requests/minute per user)\n- `503`: LLM provider unavailable (returns fallback category-based recommendations)\n\n---\n\n## 6. Data Model\n\n### RecommendationRequest\n| Field | Type | Required | Description |\n|-------|------|----------|-------------|\n| project_id | UUID | Yes | Associated project |\n| context_description | TEXT | Yes | User-provided project description |\n| category_hint | VARCHAR(50) | No | Optional category filter |\n| document_summaries | JSONB | No | Array of document name + summary pairs |\n| created_at | TIMESTAMP | Yes | Request timestamp |\n\n### RecommendationResponse\n| Field | Type | Required | Description |\n|-------|------|----------|-------------|\n| id | UUID | Yes | Response identifier |\n| request_id | UUID | Yes | FK to request |\n| framework_id | VARCHAR(100) | Yes | Matched framework ID |\n| confidence | DECIMAL(3,2) | Yes | Score between 0.00 and 1.00 |\n| reasoning | TEXT | Yes | LLM-generated explanation |\n| accepted | BOOLEAN | No | Whether user selected this recommendation |\n\n---\n\n## 7. Performance & Scalability\n\n- **Latency Target**: P95 < 2,000ms (including LLM call)\n- **Throughput**: Support 100 concurrent requests\n- **Caching**: Redis cache reduces LLM calls by ~30%; TTL of 1 hour\n- **Rate Limiting**: 10 requests/minute per user to control API costs\n- **Estimated Monthly Cost**: ~$450/month at 10,000 MAU (based on average 3 recommendations per user per month)\n\n---\n\n## 8. Security Considerations\n\n- All project context is transmitted over HTTPS\n- LLM API keys stored in environment variables, never in code\n- User project descriptions are not logged in plaintext (hashed for cache keys)\n- Rate limiting prevents abuse and cost overruns\n- Document content summaries are truncated client-side before transmission\n\n---\n\n## 9. Testing Strategy\n\n- **Unit Tests**: Prompt Builder output format, Response Parser edge cases, Framework Matcher validation\n- **Integration Tests**: End-to-end recommendation flow with mocked LLM responses\n- **Load Tests**: Simulate 100 concurrent users to validate latency targets\n- **Quality Tests**: Curated set of 50 project descriptions with expected framework matches; target 80%+ accuracy\n\n---\n\n## 10. Rollout Plan\n\n1. **Week 1**: Deploy behind feature flag to internal team (5 users)\n2. **Week 2**: Enable for 10% of beta users, monitor latency and accuracy\n3. **Week 3**: Ramp to 50% with A/B test (recommendation vs. manual browse)\n4. **Week 4**: Full rollout if acceptance rate exceeds 60%\n\n---\n\n## 11. Open Questions\n\n1. Should we support streaming responses for the reasoning text?\n2. What is the fallback behavior if Redis cache is unavailable?\n3. Do we need to store recommendation history for analytics beyond 90 days?",
  "system_prompt": "You are a senior software engineer writing a technical specification document for a product feature.\n\nBased on the provided context, create a detailed technical spec that serves as the engineering blueprint for implementation. This document bridges the gap between the product requirements (PRD) and the actual code.\n\n## Structure:\n\n1. **Summary**: Brief description of the feature, author, reviewers, status, and last updated date.\n\n2. **Background & Motivation**: Why this is being built. Link to PRD or user research. Include quantitative data where available.\n\n3. **Goals & Non-Goals**: Technical goals (latency, throughput, accuracy) and explicit non-goals to constrain scope.\n\n4. **Architecture Overview**: High-level system design. Describe components, their responsibilities, and how they interact. Include text-based diagrams where helpful.\n\n5. **API Design**: Define endpoints, request/response schemas (JSON), error codes, and rate limits. Use concrete examples with realistic data.\n\n6. **Data Model**: Tables, fields, types, relationships, and indexes. Use table format for clarity.\n\n7. **Performance & Scalability**: Latency targets, throughput expectations, caching strategy, and cost estimates.\n\n8. **Security Considerations**: Authentication, authorization, data handling, encryption, and compliance requirements.\n\n9. **Testing Strategy**: Unit, integration, load, and quality testing approaches.\n\n10. **Rollout Plan**: Phased deployment strategy with feature flags, percentage ramps, and success criteria.\n\n11. **Open Questions**: Unresolved technical decisions that need input from the team.\n\n## Guidelines:\n- Be precise with numbers (latency in ms, throughput in requests/second)\n- Include realistic code snippets and JSON examples\n- Address failure modes and fallback behaviors\n- Consider backward compatibility and migration paths\n- Keep the document actionable for a mid-level engineer to implement from",
  "guiding_questions": [
    "What feature or system are you designing?",
    "What are the key technical requirements and constraints?",
    "What existing systems or APIs does this integrate with?",
    "What are the performance targets (latency, throughput, availability)?",
    "What is the expected scale (users, requests, data volume)?",
    "Are there security or compliance requirements to consider?",
    "What is the desired rollout strategy (feature flags, phased release)?"
  ],
  "supports_visuals": false
}