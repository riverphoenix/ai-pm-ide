{
  "id": "definition-of-done",
  "name": "Definition of Done",
  "category": "development",
  "description": "Establish clear completion criteria for stories, sprints, and releases",
  "icon": "\u2705",
  "example_output": "# Definition of Done: AI PM Workspace\n\n**Team**: PM Workspace Engineering\n**Last Reviewed**: February 17, 2026\n**Next Review**: March 31, 2026 (quarterly)\n**Approved By**: David Park (Tech Lead), Dimitris Sotiriou (Product Director), Tom Wilson (QA Lead)\n\n---\n\n## Why This Matters\n\nA shared Definition of Done (DoD) ensures every team member has the same understanding of what \"complete\" means. It prevents incomplete work from flowing downstream, reduces rework, and builds trust between engineering, product, and design. This document applies to all work delivered by the PM Workspace engineering team.\n\n---\n\n## Story-Level Definition of Done\n\nEvery user story must meet ALL of the following criteria before it can be moved to \"Done\":\n\n### Code Quality\n- [ ] Code is written and follows the team's style guide and linting rules (ESLint + Prettier, zero warnings)\n- [ ] Code has been peer-reviewed by at least one other engineer (approved PR on GitHub)\n- [ ] No TODO or FIXME comments left in production code without a linked tracking ticket\n- [ ] Functions and modules have clear naming; no abbreviations or ambiguous variable names\n- [ ] Dead code and unused imports have been removed\n\n### Testing\n- [ ] Unit tests written for all new functions and methods\n- [ ] Unit test coverage for new code is at or above 80%\n- [ ] Integration tests written for any new API endpoints or service interactions\n- [ ] All existing tests pass (zero failures in CI pipeline)\n- [ ] Edge cases and error scenarios have test coverage (null inputs, timeouts, invalid data)\n- [ ] Manual QA completed by the QA engineer or a peer (not the story author)\n\n### Functionality\n- [ ] Acceptance criteria from the user story are met and verified\n- [ ] Feature works correctly in all supported browsers (Chrome, Firefox, Safari, Edge)\n- [ ] Feature is responsive and functional on screen widths from 1024px to 2560px\n- [ ] Loading states, empty states, and error states are handled gracefully\n- [ ] Keyboard navigation works for all interactive elements (WCAG 2.1 AA compliance)\n\n### Performance\n- [ ] No new API calls with response times exceeding 500ms (P95)\n- [ ] No UI interactions with perceptible lag (above 100ms response time)\n- [ ] Bundle size increase is under 50KB for any single feature addition\n- [ ] No memory leaks detected in a 30-minute usage session (Chrome DevTools profiling)\n\n### Documentation\n- [ ] API changes are reflected in the OpenAPI specification\n- [ ] Complex logic has inline code comments explaining the \"why\" (not the \"what\")\n- [ ] README updated if setup steps or environment variables changed\n- [ ] Release notes draft written for user-facing changes\n\n### Deployment\n- [ ] Code is merged to the main branch via squash merge\n- [ ] CI/CD pipeline passes all stages (lint, test, build, security scan)\n- [ ] Feature flag is configured if the feature requires gradual rollout\n- [ ] Database migrations (if any) are backward-compatible and tested\n- [ ] Rollback procedure is documented for high-risk changes\n\n---\n\n## Sprint-Level Definition of Done\n\nA sprint is considered \"Done\" when:\n\n- [ ] All committed stories (High and Medium priority) meet the Story-Level DoD above\n- [ ] Sprint goal statement is achieved as validated by the Product Manager\n- [ ] No P1 (critical) or P2 (high) bugs remain open from sprint work\n- [ ] Sprint demo is prepared and delivered to stakeholders\n- [ ] Sprint retrospective is conducted and action items are logged\n- [ ] Velocity and burndown data are updated in the project tracker\n- [ ] Any carryover items are documented with reasons and re-estimated for the next sprint\n\n---\n\n## Release-Level Definition of Done\n\nA release is considered ready for production when:\n\n- [ ] All sprint-level DoD criteria are met for all included sprints\n- [ ] End-to-end regression test suite passes with zero failures\n- [ ] Performance benchmark tests show no degradation beyond 5% from the previous release\n- [ ] Security scan (SAST + dependency audit) returns zero critical or high vulnerabilities\n- [ ] Product Manager has signed off on the feature set and user experience\n- [ ] QA Lead has signed off on test coverage and quality\n- [ ] Release notes are finalized and published\n- [ ] Monitoring and alerting are configured for new features (error rates, latency, usage)\n- [ ] On-call team is briefed on new features and potential failure modes\n- [ ] Rollback plan is documented and tested in staging\n\n---\n\n## Exceptions Process\n\nIf a story cannot meet all DoD criteria, the following process applies:\n\n1. The engineer raises the exception in daily standup with a specific reason\n2. The Tech Lead and Product Manager jointly decide whether to grant an exception\n3. If granted, a follow-up ticket is created immediately to address the gap\n4. The exception and follow-up ticket are documented in the sprint retrospective\n5. No more than 2 exceptions are allowed per sprint; if exceeded, the team must discuss systemic issues\n\n---\n\n## Review Cadence\n\nThis Definition of Done is reviewed quarterly by the full team. Any team member can propose changes via a pull request to this document. Changes require approval from the Tech Lead and Product Manager.",
  "system_prompt": "You are an experienced Agile coach creating a Definition of Done (DoD) document for a product development team.\n\nBased on the provided context, create a detailed and practical DoD that the team will use as their quality gate for all work. The DoD should be specific enough to be actionable but not so rigid that it slows the team down unnecessarily.\n\n## Structure:\n\n1. **Header**: Team name, last reviewed date, next review date, and approvers.\n\n2. **Why This Matters**: A brief paragraph explaining the purpose of the DoD and why every team member should care about it.\n\n3. **Story-Level Definition of Done**: Checklist of criteria that EVERY user story must meet before it can be marked complete. Organize into categories:\n   - **Code Quality**: Style guide adherence, code review, no dead code\n   - **Testing**: Unit tests, integration tests, coverage thresholds, manual QA\n   - **Functionality**: Acceptance criteria met, cross-browser, responsive, accessibility\n   - **Performance**: Latency targets, bundle size limits, no memory leaks\n   - **Documentation**: API docs, inline comments, README updates, release notes\n   - **Deployment**: CI/CD pipeline, feature flags, migrations, rollback plan\n\n4. **Sprint-Level Definition of Done**: Criteria for the sprint itself to be considered complete (goal achieved, no critical bugs, demo delivered, retro conducted).\n\n5. **Release-Level Definition of Done**: Criteria for production readiness (regression tests, security scans, stakeholder sign-off, monitoring configured, rollback plan tested).\n\n6. **Exceptions Process**: How the team handles cases where a DoD criterion cannot be met. Include escalation path and follow-up requirements.\n\n7. **Review Cadence**: How often the DoD is reviewed and updated, and the process for proposing changes.\n\n## Guidelines:\n- Use checkbox format (- [ ]) for all criteria so the team can use this as a literal checklist\n- Be specific with numbers (coverage percentages, latency thresholds, response times)\n- Tailor criteria to the team's technology stack and workflow\n- Balance thoroughness with pragmatism; avoid criteria that add overhead without clear value\n- Include both functional and non-functional quality criteria\n- Make the DoD achievable; overly strict criteria that are routinely ignored become meaningless",
  "guiding_questions": [
    "What is your team's technology stack (languages, frameworks, tools)?",
    "What is your current testing strategy and coverage expectations?",
    "What are your performance and quality standards?",
    "What deployment and release process do you follow?",
    "Are there compliance, security, or accessibility requirements?",
    "What quality issues have you experienced recently that the DoD should prevent?",
    "How large is the team and what roles are involved in the review process?"
  ],
  "supports_visuals": false
}