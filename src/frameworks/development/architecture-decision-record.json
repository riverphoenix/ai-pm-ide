{
  "id": "architecture-decision-record",
  "name": "Architecture Decision Record (ADR)",
  "category": "development",
  "description": "Document architectural decisions with context, options considered, and rationale",
  "icon": "\ud83c\udfd7\ufe0f",
  "example_output": "# ADR-007: Use Redis for Framework Recommendation Caching\n\n## Status\n**Accepted** | February 17, 2026\n\n**Deciders**: David Park (Tech Lead), Sarah Lopez (Backend Engineer), James Kim (Frontend)\n**Consulted**: Maria Chen (Full-Stack), Product Team\n**Informed**: Engineering org\n\n---\n\n## Context\n\nThe AI PM Workspace's new framework recommendation engine calls an LLM API (Claude) for every recommendation request. Each call costs approximately $0.03-0.05 and takes 1-2 seconds. With a projected 10,000 monthly active users making an average of 3 recommendation requests each, this results in:\n\n- **30,000 LLM API calls per month** ($900-$1,500/month in API costs)\n- **User-facing latency of 1-2 seconds** on every request\n- **No benefit from repeated queries**: Users with similar project descriptions get no speed advantage\n\nAnalysis of beta usage data shows that approximately 35% of recommendation requests share substantially similar context (same category, similar descriptions). A caching layer could significantly reduce costs and latency for these repeated patterns.\n\n---\n\n## Decision\n\nWe will use **Redis** as an in-memory caching layer for framework recommendation responses, keyed on a SHA-256 hash of the normalized project context. Cache entries will have a TTL of 1 hour.\n\n---\n\n## Options Considered\n\n### Option 1: Redis (In-Memory Cache) -- SELECTED\n\n**Description**: Deploy a managed Redis instance (AWS ElastiCache) to cache LLM responses. Cache key is a hash of the normalized input context. Cache value is the full recommendation response JSON.\n\n**Pros**:\n- Sub-millisecond read latency (P99 < 1ms)\n- Mature, battle-tested technology with excellent tooling\n- Native TTL support for automatic expiration\n- AWS ElastiCache provides managed scaling and failover\n- Team has existing Redis operational experience\n\n**Cons**:\n- Additional infrastructure cost (~$50/month for cache.t3.micro)\n- Data is volatile (lost on restart, but acceptable for cache)\n- Requires cache invalidation strategy when frameworks are updated\n\n**Estimated Cost**: $50/month (ElastiCache) + 2 engineering days to implement\n\n### Option 2: Application-Level In-Memory Cache (e.g., Node.js Map / LRU Cache)\n\n**Description**: Use an in-process LRU cache within the application server.\n\n**Pros**:\n- Zero additional infrastructure\n- Simplest implementation (npm package, 10 lines of code)\n- No network hop for cache reads\n\n**Cons**:\n- Cache is per-instance (not shared across multiple servers)\n- Lost on every deployment or restart\n- Cannot scale horizontally (each instance has its own cache)\n- Memory pressure on application server\n\n**Estimated Cost**: $0 infrastructure + 0.5 engineering days\n\n### Option 3: PostgreSQL Materialized View / Table\n\n**Description**: Store recommendation results in a PostgreSQL table with an expiry column. Query the table before calling the LLM.\n\n**Pros**:\n- Uses existing database infrastructure\n- Persistent across restarts\n- Easy to query and analyze cached data\n\n**Cons**:\n- Higher read latency (5-20ms vs. sub-1ms for Redis)\n- Requires manual cleanup job for expired entries\n- Adds load to primary database\n- Over-engineered for a simple cache use case\n\n**Estimated Cost**: $0 infrastructure + 3 engineering days\n\n### Option 4: No Caching\n\n**Description**: Call the LLM API for every recommendation request.\n\n**Pros**:\n- Simplest architecture\n- Always returns fresh results\n- No cache invalidation complexity\n\n**Cons**:\n- Highest API costs ($900-$1,500/month at projected scale)\n- No latency improvement for repeated queries\n- Wastes API calls on identical or near-identical inputs\n\n**Estimated Cost**: $900-$1,500/month in ongoing API costs\n\n---\n\n## Decision Rationale\n\nRedis was selected because it provides the best balance of performance, reliability, and operational simplicity:\n\n1. **Cost savings**: Projected 30-35% reduction in LLM API calls saves $270-$525/month, far exceeding the $50/month Redis cost\n2. **Latency improvement**: Cache hits return in <1ms vs. 1-2 seconds from the LLM, dramatically improving UX for common queries\n3. **Horizontal scaling**: Shared cache works across multiple application instances, supporting our planned scaling to 3 servers in Q3\n4. **Operational maturity**: Team has 2+ years of Redis experience from the session management system; no learning curve\n5. **Managed service**: AWS ElastiCache handles failover, patching, and monitoring\n\nOption 2 (in-process cache) was the runner-up but was eliminated because we plan to run multiple application instances by Q3, and per-instance caches would result in low hit rates and duplicated memory usage.\n\n---\n\n## Consequences\n\n**Positive**:\n- 30-35% reduction in LLM API costs\n- Sub-millisecond response times for cached recommendations\n- Foundation for caching other LLM-powered features in the future\n\n**Negative**:\n- Additional infrastructure dependency (Redis must be monitored and maintained)\n- Cache invalidation needed when framework library is updated (planned: invalidate all on framework deploy)\n- Slightly stale recommendations possible within the 1-hour TTL window\n\n**Risks**:\n- If Redis goes down, the system must gracefully fall back to direct LLM calls (implemented as try/catch with logging)\n- Hash collisions are theoretically possible but negligible with SHA-256\n\n---\n\n## Follow-Up Actions\n\n- [ ] Sarah L.: Implement Redis caching layer (Sprint 14, PM-210)\n- [ ] David P.: Set up ElastiCache instance in staging environment\n- [ ] Tom W.: Add cache hit/miss metrics to monitoring dashboard\n- [ ] Sarah L.: Implement cache invalidation hook on framework library updates\n\n---\n\n## References\n\n- Technical Spec: AI Framework Recommendation Engine\n- AWS ElastiCache Pricing: https://aws.amazon.com/elasticache/pricing/\n- Redis Best Practices for Caching: https://redis.io/docs/manual/patterns/caching/",
  "system_prompt": "You are a senior software architect writing an Architecture Decision Record (ADR) following the widely-adopted ADR template by Michael Nygard.\n\nBased on the provided context, create a well-structured ADR that documents an architectural decision, the options considered, and the rationale for the chosen approach. ADRs are living documents that help teams understand why decisions were made.\n\n## Structure:\n\n1. **Title**: ADR-[number]: [Short decision description]. Use a noun phrase that summarizes the decision.\n\n2. **Status**: One of Draft, Proposed, Accepted, Deprecated, or Superseded. Include the date and list Deciders, Consulted, and Informed stakeholders.\n\n3. **Context**: Describe the situation and forces at play. What is the technical or business problem? Include quantitative data (costs, latency, user counts) where available. Explain why a decision is needed now.\n\n4. **Decision**: State the decision clearly in 1-2 sentences. Be direct and unambiguous.\n\n5. **Options Considered**: For each option (including the selected one), provide:\n   - Description of the approach\n   - Pros (3-5 bullet points)\n   - Cons (3-5 bullet points)\n   - Estimated cost (infrastructure + engineering effort)\n   Mark the selected option clearly.\n\n6. **Decision Rationale**: Explain why the selected option was chosen over alternatives. Reference specific advantages and how they outweigh the tradeoffs. Number the key reasons.\n\n7. **Consequences**: Break into Positive, Negative, and Risks. Be honest about tradeoffs and downstream impacts.\n\n8. **Follow-Up Actions**: Concrete action items with owners to implement the decision.\n\n9. **References**: Links to related documents, specs, and external resources.\n\n## Guidelines:\n- Present at least 3 options (including \"do nothing\" where applicable)\n- Be objective in pros/cons analysis; avoid biasing the comparison\n- Include cost estimates for each option\n- Document the decision even if it seems obvious today; future team members need the context\n- Keep the tone neutral and factual\n- Use concrete numbers over vague qualifiers",
  "guiding_questions": [
    "What architectural decision needs to be made?",
    "What is the technical or business context driving this decision?",
    "What options or alternatives have been considered?",
    "What are the key evaluation criteria (cost, performance, complexity, team expertise)?",
    "Who are the decision-makers and stakeholders?",
    "Are there any constraints or non-negotiable requirements?",
    "What is the timeline for making and implementing this decision?"
  ],
  "supports_visuals": false
}