{
  "id": "weighted-scoring",
  "name": "Weighted Scoring Model",
  "category": "prioritization",
  "description": "Score features against weighted criteria to produce a transparent, defensible ranking",
  "icon": "ðŸ”¢",
  "example_output": "# Weighted Scoring Model Analysis\n\n## Context\n**Product**: PM IDE - AI-Powered Product Management Tool\n**Objective**: Rank 6 candidate features for H2 2026 roadmap prioritization\n**Stakeholders**: Product leadership, Engineering leads, Customer Success\n**Methodology**: Multi-criteria weighted scoring with stakeholder-aligned weights\n\n---\n\n## Step 1: Define Criteria & Weights\n\nCriteria were selected based on the product's current strategic priorities and validated with stakeholders. Weights sum to 100%.\n\n| # | Criterion | Description | Weight | Rationale |\n|---|-----------|-------------|--------|-----------|\n| C1 | User Demand | Strength of user signal from interviews, surveys, and support tickets | 25% | We are in a product-market fit refinement phase. Listening to users is paramount. |\n| C2 | Strategic Alignment | How well the feature supports our AI-first product positioning and H2 OKRs | 25% | Half of our OKRs are AI-related. Features that advance the AI narrative get extra weight. |\n| C3 | Revenue Impact | Potential to drive conversions, reduce churn, or enable pricing tier upgrades | 20% | Business sustainability requires revenue-conscious prioritization. |\n| C4 | Implementation Feasibility | Ease of implementation considering current architecture, team skills, and timeline | 15% | Feasibility acts as a reality check. High-value but infeasible items waste planning cycles. |\n| C5 | Competitive Differentiation | Degree to which the feature creates a unique advantage vs. alternatives | 15% | In a crowded market, undifferentiated features are commoditized quickly. |\n\n**Total Weight: 100%**\n\n---\n\n## Step 2: Score Each Feature (1-10 Scale)\n\n### Scoring Rubric\n| Score | Meaning |\n|-------|---------|\n| 9-10 | Exceptional. Top-tier evidence or impact. |\n| 7-8 | Strong. Clear evidence or significant impact. |\n| 5-6 | Moderate. Some evidence or noticeable impact. |\n| 3-4 | Weak. Limited evidence or minor impact. |\n| 1-2 | Negligible. No evidence or trivial impact. |\n\n### Feature: AI-Powered Framework Recommendations\n*The AI analyzes the user's problem description and suggests the most appropriate framework to use.*\n\n| Criterion | Score | Evidence / Rationale |\n|-----------|-------|---------------------|\n| C1: User Demand | 6 | Not directly requested, but 8 users mentioned \"I didn't know which framework to pick.\" Latent need. |\n| C2: Strategic Alignment | 10 | Directly embodies our AI-first positioning. This IS the product vision. |\n| C3: Revenue Impact | 7 | Reduces time-to-value, which our data shows correlates with conversion (r=0.64). |\n| C4: Feasibility | 6 | Requires new intent classification pipeline. Moderate complexity. 4-6 weeks. |\n| C5: Differentiation | 9 | No competitor offers this. Clear first-mover advantage. |\n\n### Feature: PDF & Slide Export\n*Export any framework analysis as a formatted PDF or presentation slide deck.*\n\n| Criterion | Score | Evidence / Rationale |\n|-----------|-------|---------------------|\n| C1: User Demand | 9 | #1 most-requested feature. 14 of 24 interview participants asked for this unprompted. |\n| C2: Strategic Alignment | 4 | Useful but not AI-related. Infrastructure feature, not strategic differentiator. |\n| C3: Revenue Impact | 6 | Removes a friction point in sharing. Indirect impact on team adoption and expansion revenue. |\n| C4: Feasibility | 8 | Well-understood libraries (Puppeteer, reveal.js). 2-3 weeks. |\n| C5: Differentiation | 3 | Table stakes. Most tools offer export. Not a differentiator. |\n\n### Feature: Real-Time Collaboration\n*Multiple users can edit a framework analysis simultaneously with live cursors and comments.*\n\n| Criterion | Score | Evidence / Rationale |\n|-----------|-------|---------------------|\n| C1: User Demand | 5 | 4 users mentioned wanting to \"work together.\" Moderate signal. |\n| C2: Strategic Alignment | 6 | Supports team workflows but not directly AI-related. |\n| C3: Revenue Impact | 8 | Multiplayer features are the #1 driver of seat expansion in SaaS. Strong revenue lever. |\n| C4: Feasibility | 2 | Requires CRDT/OT infrastructure, WebSocket layer, permission system. 3+ months. Major undertaking. |\n| C5: Differentiation | 7 | Few PM tools offer real-time co-editing on analysis frameworks. Moderate differentiation. |\n\n### Feature: Framework Template Marketplace\n*Community-contributed framework templates that users can browse, rate, and install.*\n\n| Criterion | Score | Evidence / Rationale |\n|-----------|-------|---------------------|\n| C1: User Demand | 4 | 2 users mentioned wanting custom frameworks. No direct marketplace demand. |\n| C2: Strategic Alignment | 7 | Builds ecosystem and community. Supports long-term platform strategy. |\n| C3: Revenue Impact | 5 | Could drive viral adoption through shared templates. Speculative but plausible. |\n| C4: Feasibility | 4 | Requires content moderation, rating system, install mechanism. 6-8 weeks. |\n| C5: Differentiation | 8 | No competitor has a framework marketplace. Novel concept for the category. |\n\n### Feature: Historical Analysis Dashboard\n*View all past analyses in a searchable dashboard with trend comparisons over time.*\n\n| Criterion | Score | Evidence / Rationale |\n|-----------|-------|---------------------|\n| C1: User Demand | 7 | 6 users asked \"where did my old analysis go?\" Clear need for retrieval. |\n| C2: Strategic Alignment | 5 | Supports user retention and habit formation. Indirectly strategic. |\n| C3: Revenue Impact | 5 | Increases stickiness. Users with history are less likely to churn. Moderate impact. |\n| C4: Feasibility | 7 | Requires indexing and search on existing data. 2-3 weeks. Straightforward. |\n| C5: Differentiation | 4 | Some competitors offer history views. Minor differentiation. |\n\n### Feature: Automated Stakeholder Report\n*AI generates a summary email or Slack message of key decisions from a completed analysis, ready to send.*\n\n| Criterion | Score | Evidence / Rationale |\n|-----------|-------|---------------------|\n| C1: User Demand | 5 | 3 users mentioned wanting easier sharing. Not a top request. |\n| C2: Strategic Alignment | 8 | AI-generated content aligns with AI-first positioning. Extends AI beyond analysis into communication. |\n| C3: Revenue Impact | 4 | Nice-to-have. Unlikely to directly drive conversions or prevent churn. |\n| C4: Feasibility | 7 | Leverages existing LLM pipeline. Slack/email APIs well-documented. 2 weeks. |\n| C5: Differentiation | 7 | Unique in the PM tool space. Bridges analysis and communication. |\n\n---\n\n## Step 3: Calculate Weighted Scores\n\n| Feature | C1 (25%) | C2 (25%) | C3 (20%) | C4 (15%) | C5 (15%) | **Total** |\n|---------|----------|----------|----------|----------|----------|----------|\n| AI Framework Recommendations | 6 x 0.25 = 1.50 | 10 x 0.25 = 2.50 | 7 x 0.20 = 1.40 | 6 x 0.15 = 0.90 | 9 x 0.15 = 1.35 | **7.65** |\n| PDF & Slide Export | 9 x 0.25 = 2.25 | 4 x 0.25 = 1.00 | 6 x 0.20 = 1.20 | 8 x 0.15 = 1.20 | 3 x 0.15 = 0.45 | **6.10** |\n| Real-Time Collaboration | 5 x 0.25 = 1.25 | 6 x 0.25 = 1.50 | 8 x 0.20 = 1.60 | 2 x 0.15 = 0.30 | 7 x 0.15 = 1.05 | **5.70** |\n| Framework Marketplace | 4 x 0.25 = 1.00 | 7 x 0.25 = 1.75 | 5 x 0.20 = 1.00 | 4 x 0.15 = 0.60 | 8 x 0.15 = 1.20 | **5.55** |\n| Historical Dashboard | 7 x 0.25 = 1.75 | 5 x 0.25 = 1.25 | 5 x 0.20 = 1.00 | 7 x 0.15 = 1.05 | 4 x 0.15 = 0.60 | **5.65** |\n| Automated Stakeholder Report | 5 x 0.25 = 1.25 | 8 x 0.25 = 2.00 | 4 x 0.20 = 0.80 | 7 x 0.15 = 1.05 | 7 x 0.15 = 1.05 | **6.15** |\n\n---\n\n## Step 4: Final Ranking\n\n| Rank | Feature | Weighted Score | Tier |\n|------|---------|---------------|------|\n| 1 | AI-Powered Framework Recommendations | **7.65** | Tier 1 - Commit |\n| 2 | Automated Stakeholder Report | **6.15** | Tier 1 - Commit |\n| 3 | PDF & Slide Export | **6.10** | Tier 1 - Commit |\n| 4 | Real-Time Collaboration | **5.70** | Tier 2 - Plan |\n| 5 | Historical Analysis Dashboard | **5.65** | Tier 2 - Plan |\n| 6 | Framework Template Marketplace | **5.55** | Tier 3 - Backlog |\n\n---\n\n## Sensitivity Analysis\n\nTo test whether the ranking is robust, I adjusted the weights for two alternate scenarios:\n\n**Scenario A: Revenue-First** (C3 weight increased to 35%, C2 reduced to 15%)\n| Rank | Feature | Score |\n|------|---------|-------|\n| 1 | Real-Time Collaboration | 5.90 |\n| 2 | AI Framework Recommendations | 6.95 |\n| 3 | PDF & Slide Export | 6.30 |\n\n**Scenario B: User-First** (C1 weight increased to 40%, C5 reduced to 5%)\n| Rank | Feature | Score |\n|------|---------|-------|\n| 1 | PDF & Slide Export | 6.55 |\n| 2 | AI Framework Recommendations | 7.05 |\n| 3 | Historical Dashboard | 5.85 |\n\n**Key finding**: AI Framework Recommendations remains in the top 2 across all scenarios. It is the most robust pick. PDF & Slide Export and Automated Stakeholder Report are also stable. Real-Time Collaboration is highly sensitive to revenue weighting - only rises when revenue is the dominant criterion.\n\n---\n\n## Recommendation\n\n**Tier 1 (Commit - H2 2026)**: AI Framework Recommendations, Automated Stakeholder Report, PDF Export. These three score highest and are feasible within the half.\n\n**Tier 2 (Plan - H1 2027)**: Real-Time Collaboration, Historical Dashboard. Begin design work in H2 but defer build to next half. Collaboration needs a technical spike to de-risk the CRDT approach.\n\n**Tier 3 (Backlog)**: Framework Marketplace. Promising concept but needs more user validation before committing resources. Run a survey or concept test in H2.\n\nThe weighted scoring model confirms that our AI-first strategy should drive prioritization. The top-ranked feature (AI Framework Recommendations) scores highest precisely because our weights reflect our strategic bet on AI. This is the model working as intended - it translates strategy into actionable prioritization.",
  "system_prompt": "You are a product prioritization expert specializing in multi-criteria weighted scoring models. Your task is to help product managers create a transparent, defensible, and repeatable prioritization framework using weighted criteria.\n\nThe weighted scoring model works in four steps:\n\n**Step 1 - Define Criteria & Weights**: Help the user identify 4-6 evaluation criteria relevant to their product context. Common criteria include: user demand, strategic alignment, revenue impact, implementation feasibility, competitive differentiation, and technical risk. Each criterion receives a percentage weight reflecting its relative importance. Weights must sum to 100%. Challenge the user if weights do not reflect their stated strategy - for example, if they claim to be \"AI-first\" but give strategic alignment only 10% weight, flag the misalignment.\n\n**Step 2 - Score Each Feature**: For each feature, assign a score of 1-10 on each criterion. Provide a clear scoring rubric (e.g., 9-10 = exceptional, 7-8 = strong, 5-6 = moderate, 3-4 = weak, 1-2 = negligible). Every score MUST be accompanied by explicit evidence or rationale. Scores without justification are not acceptable. When scoring, be calibrated: avoid clustering all scores in the 6-8 range. Use the full 1-10 scale to create meaningful differentiation between features.\n\n**Step 3 - Calculate Weighted Scores**: For each feature, multiply each criterion score by the criterion weight, then sum across all criteria to produce a total weighted score (range: 1.0 to 10.0). Show the calculation explicitly so stakeholders can verify the math and understand how each criterion contributes to the total.\n\n**Step 4 - Rank and Recommend**: Sort features by weighted score. Group them into commitment tiers (Commit, Plan, Backlog). Run a sensitivity analysis by testing 1-2 alternative weight scenarios to verify the ranking is robust. If the ranking changes significantly under reasonable alternative weights, flag this as a risk.\n\nStructure your output with these sections:\n1. Context (product, objective, stakeholders, methodology summary)\n2. Criteria definition table with weights and rationale\n3. Scoring rubric\n4. Detailed scoring for each feature (one table per feature with criterion, score, and evidence)\n5. Weighted score calculation table showing all features, all criteria contributions, and totals\n6. Final ranking table with tiers\n7. Sensitivity analysis under alternative weight scenarios\n8. Recommendation summarizing what to commit, plan, and backlog\n\nCritical guidelines:\n- The model's power is in transparency. Show ALL math. Stakeholders must be able to trace any score back to its evidence and weight.\n- Weights encode strategy. The first conversation should be about weights, not scores. Get weights right before scoring anything.\n- Beware of gaming. If a user wants to adjust scores to get a predetermined outcome, push back. The model's value is in honest scoring.\n- Always run sensitivity analysis. A ranking that collapses under slight weight changes is not trustworthy.\n- Use markdown with detailed tables, bold text, and clear section headers.",
  "guiding_questions": [
    "What features or initiatives are you evaluating?",
    "What criteria matter most for your product decisions right now (user demand, revenue, strategy, feasibility, differentiation)?",
    "How would you weight these criteria relative to each other?",
    "What evidence do you have for each feature (user research, data, business cases)?",
    "Who are the stakeholders that need to buy into this prioritization?",
    "Are there any constraints that should be reflected in the criteria (budget, timeline, technical debt)?",
    "What is the planning horizon for this prioritization exercise?"
  ],
  "supports_visuals": false
}
