{
  "id": "ice-scoring",
  "name": "ICE Scoring",
  "category": "prioritization",
  "description": "Score and rank features using Impact, Confidence, and Ease on a 1-10 scale",
  "icon": "ðŸ§Š",
  "example_output": "# ICE Scoring Analysis\n\n## Context\n**Product**: PM IDE - AI-Powered Product Management Tool\n**Goal**: Prioritize backlog items for the next 6-week development cycle\n**Team**: 3 engineers, 1 designer\n**Scoring Scale**: 1-10 for each dimension (Impact, Confidence, Ease)\n**ICE Score**: Impact x Confidence x Ease (max 1000)\n\n---\n\n## Scoring Criteria Definitions\n\n| Score | Impact | Confidence | Ease |\n|-------|--------|------------|------|\n| 10 | Transforms the product. Step-change in key metric. | Proven by A/B test or strong quantitative data. | Can ship in under 1 day. Trivial change. |\n| 8 | Major improvement to a core workflow. Measurable lift. | Strong qualitative evidence (multiple interviews, clear pattern). | 1-3 days of work. Well-understood scope. |\n| 6 | Noticeable improvement for a meaningful user segment. | Moderate evidence. Some data, some assumption. | 1-2 weeks. Some unknowns but manageable. |\n| 4 | Minor improvement. Affects a small segment or edge case. | Limited evidence. Mostly intuition with some signal. | 2-4 weeks. Significant unknowns or dependencies. |\n| 2 | Marginal or unclear impact. Hard to measure. | Speculative. Little to no supporting evidence. | 1-2 months. Major effort, new technology, or external dependencies. |\n| 1 | No measurable impact expected. | Pure guess. | 3+ months. Requires fundamental architecture changes. |\n\n---\n\n## Feature Scoring\n\n### 1. Add RICE Framework Template\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| **Impact** | 8 | RICE is the most-requested framework in user feedback. Adding it fills a critical gap and directly increases the tool's utility for the primary use case. |\n| **Confidence** | 9 | 12 of 24 interviewed users specifically asked for RICE. Support tickets confirm demand. We have a working template engine, so the pattern is proven. |\n| **Ease** | 9 | Template engine already exists. This is a JSON definition file plus prompt engineering. Estimated 2 days of work. |\n\n**ICE Score: 648**\n\n---\n\n### 2. AI-Powered Framework Recommendation\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| **Impact** | 9 | Would fundamentally change the onboarding experience. Instead of users choosing from a list, the AI guides them to the right framework. Reduces time-to-value significantly. |\n| **Confidence** | 5 | No direct user feedback requesting this. Based on product intuition and analogies from other AI tools. Prototype untested. |\n| **Ease** | 6 | Requires building an intent classification layer. Moderate complexity - can leverage existing LLM but needs new prompt pipeline and UI flow. ~2 weeks. |\n\n**ICE Score: 270**\n\n---\n\n### 3. PDF Export\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| **Impact** | 7 | Users need to share analyses with stakeholders. PDF is the universal sharing format. Currently users copy-paste into Google Docs, which is friction-heavy. |\n| **Confidence** | 8 | 8 users requested this directly. Clear pain point observed in usability sessions - users struggle to share outputs. |\n| **Ease** | 7 | Well-understood problem. Libraries like Puppeteer/wkhtmltopdf can convert markdown to PDF. ~1 week including formatting polish. |\n\n**ICE Score: 392**\n\n---\n\n### 4. Collaborative Editing (Real-Time)\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| **Impact** | 7 | Product decisions are collaborative. Real-time editing removes the share-wait-merge cycle. Could meaningfully increase team adoption. |\n| **Confidence** | 4 | Only 3 users mentioned collaboration. Most seem satisfied with export-and-share. Impact is assumed, not validated. |\n| **Ease** | 2 | Requires WebSocket infrastructure, conflict resolution (CRDT or OT), presence indicators, and permission models. 2-3 months minimum. |\n\n**ICE Score: 56**\n\n---\n\n### 5. Custom Scoring Weights in Frameworks\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| **Impact** | 5 | Power users want to adjust scoring formulas. Increases flexibility but only relevant for advanced users (~20% of base). |\n| **Confidence** | 7 | 5 users requested this. Clear pattern in feedback: \"I want to weight Impact more than Effort in my RICE scores.\" |\n| **Ease** | 8 | Framework JSON schema already supports extensibility. Adding a weight parameter and UI slider is straightforward. ~3 days. |\n\n**ICE Score: 280**\n\n---\n\n### 6. Dark Mode\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| **Impact** | 3 | Cosmetic improvement. Does not change functionality. Some accessibility benefit for light-sensitive users. |\n| **Confidence** | 6 | Commonly requested but rarely cited as a blocker or reason to churn. Moderate signal. |\n| **Ease** | 7 | CSS variable-based theming. Moderate effort to ensure all components look correct. ~1 week. |\n\n**ICE Score: 126**\n\n---\n\n## Ranked Results\n\n| Rank | Feature | Impact | Confidence | Ease | ICE Score |\n|------|---------|--------|------------|------|-----------|\n| 1 | Add RICE Framework Template | 8 | 9 | 9 | **648** |\n| 2 | PDF Export | 7 | 8 | 7 | **392** |\n| 3 | Custom Scoring Weights | 5 | 7 | 8 | **280** |\n| 4 | AI Framework Recommendation | 9 | 5 | 6 | **270** |\n| 5 | Dark Mode | 3 | 6 | 7 | **126** |\n| 6 | Collaborative Editing | 7 | 4 | 2 | **56** |\n\n---\n\n## Analysis & Recommendations\n\n### High-Confidence Quick Wins (ICE > 300)\n- **RICE Framework Template** (648) and **PDF Export** (392) are clear winners. Both have high confidence backed by user research and are relatively easy to implement. Prioritize these first.\n\n### Medium-Confidence Bets (ICE 200-300)\n- **Custom Scoring Weights** (280) is a solid quality-of-life improvement for power users with good confidence and ease.\n- **AI Framework Recommendation** (270) has massive impact potential but lower confidence. Consider a lightweight prototype to validate before full investment.\n\n### Defer or Validate (ICE < 200)\n- **Dark Mode** (126) is easy but low-impact. Defer unless engineering has slack time.\n- **Collaborative Editing** (56) scores poorly due to extreme implementation difficulty. The impact hypothesis also needs validation. Recommend running a design spike or user research sprint before committing.\n\n### Recommended Cycle Plan\n1. **Week 1-2**: RICE Framework Template + PDF Export (ship both)\n2. **Week 3**: Custom Scoring Weights\n3. **Week 4-5**: AI Framework Recommendation (prototype + validate)\n4. **Week 6**: Polish, bug fixes, and ship\n5. **Deferred**: Dark Mode, Collaborative Editing",
  "system_prompt": "You are a product prioritization expert specializing in ICE Scoring. Your task is to help product managers score and rank backlog items using the ICE framework: Impact, Confidence, and Ease.\n\nApply these scoring definitions rigorously:\n\n**Impact (1-10)**: How much will this feature move the needle on the target metric or goal? Score 10 for transformative, step-change improvements. Score 5 for moderate, noticeable improvements to a meaningful segment. Score 1 for negligible or unmeasurable impact. Always tie impact to a specific metric or outcome, not vague notions of \"improvement.\"\n\n**Confidence (1-10)**: How confident are you in your Impact and Ease estimates? Score 10 when you have hard data (A/B test results, quantitative research). Score 7-8 for strong qualitative evidence (consistent user interview findings, clear patterns in support data). Score 4-5 for moderate evidence mixed with assumption. Score 1-2 for pure speculation. Be brutally honest here - overconfidence is the most common ICE scoring mistake.\n\n**Ease (1-10)**: How easy is this to implement? Score 10 for trivial changes shippable in hours. Score 7-8 for well-scoped work completable in days. Score 5-6 for multi-week projects with manageable unknowns. Score 2-3 for multi-month efforts with significant technical risk. Score 1 for fundamental architecture changes. Factor in design, engineering, QA, and deployment effort.\n\n**ICE Score** = Impact x Confidence x Ease (range: 1 to 1000)\n\nStructure your output as follows:\n1. Define the context: product, goal, team, and the scoring scale being used.\n2. Present a scoring criteria reference table so the reader understands what each number means.\n3. For each feature, create a detailed scoring table with the dimension, score, and rationale for each score. Show the calculated ICE score prominently.\n4. Create a ranked results table sorting all features by ICE score from highest to lowest.\n5. Group features into tiers: High-Confidence Quick Wins (top tier), Medium-Confidence Bets (middle), and Defer or Validate (bottom).\n6. Provide a recommended execution plan showing which features to tackle in what order and why.\n\nCritical guidelines:\n- Never average the scores. ICE is multiplicative, which means a low score in ANY dimension dramatically reduces the total. This is intentional - it penalizes high-impact but low-confidence or difficult items.\n- Challenge the user's assumptions. If they rate confidence high but provide no data, push back.\n- Ease should account for total cost including design, testing, and rollout - not just engineering time.\n- When data is missing, assign conservative confidence scores and note the uncertainty.\n\nUse markdown with tables, horizontal rules, bold text, and headers. Make scores scannable at a glance.",
  "guiding_questions": [
    "What features or initiatives are you scoring?",
    "What is the primary metric or goal you are optimizing for?",
    "What evidence do you have for each feature's potential impact (data, research, intuition)?",
    "What is your team size and available capacity for this cycle?",
    "Are there any technical constraints or dependencies that affect implementation ease?",
    "How long is your planning cycle (sprint, 6 weeks, quarter)?"
  ],
  "supports_visuals": false
}
